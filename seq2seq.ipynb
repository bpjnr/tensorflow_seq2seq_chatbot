{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FnS-GXJOJOY2"
   },
   "source": [
    "Tensorflow 1.4.0 is required.\n",
    "This is based on [NMT Tutorial](https://github.com/tensorflow/nmt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "caxRbRVkDdhp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#@formatter:off\n",
    "import copy as copy\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "\n",
    "#@formatter:on\n",
    "\n",
    "\n",
    "def colab():\n",
    "    return '/tools/node/bin/forever' == os.environ['_']\n",
    "\n",
    "\n",
    "if colab():\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "# Note for myself.\n",
    "# You've summarized Seq2Seq\n",
    "# at http://d.hatena.ne.jp/higepon/20171210/1512887715.\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "def data_dir():\n",
    "    target_dir = \"{}/chatbot_data\".format(str(Path.home()))\n",
    "    if not colab() and not os.path.exists(target_dir):\n",
    "        raise Exception(\"{} not found, you may create\".format(target_dir))\n",
    "    return target_dir\n",
    "\n",
    "\n",
    "def p(path):\n",
    "    if colab():\n",
    "        return path\n",
    "    else:\n",
    "        return \"{}/{}\".format(data_dir(), path)\n",
    "\n",
    "\n",
    "def has_gpu0():\n",
    "    return tf.test.gpu_device_name() == \"/device:GPU:0\"\n",
    "\n",
    "\n",
    "os.makedirs(p(\"saved_model2\"), exist_ok=True)\n",
    "os.makedirs(p(\"saved_model\"), exist_ok=True)\n",
    "\n",
    "if True:\n",
    "  class GoogleDriveFolder(Enum):\n",
    "    root = '146ZLldWXLDH0l9WbSUNFKi3nVK_HV0Sz'\n",
    "    seq2seq = '18lYBgKvX3AG1zhwJqP1tRYJU688U1N95'\n",
    "    seq2seq_swapped = '1w56FFoKStEfZNThA2Y_jx3NDTELcau52'\n",
    "    seq2seq_rl = '1pHnOuT_7JjD1TS8VQ4KN9oUiblBIABXJ'\n",
    "else:\n",
    "  class GoogleDriveFolder(Enum):\n",
    "    root = '15Z3wbaSjR34ziPgAVEFiXgM67ln1Z9Xt'\n",
    "    seq2seq = '1KdMwLNbUfI_PZ5QTyi2zvcS___ct399p'\n",
    "    seq2seq_swapped = '1OjvR4TXAVudSiI-A7EBjw3gLoY4mrO7i'\n",
    "    seq2seq_rl = '161ler0gTpsvFUPAvc4x1__jyClJ_8IyB'  \n",
    "\n",
    "test_hparams = tf.contrib.training.HParams(\n",
    "    batch_size=3,\n",
    "    encoder_length=5,\n",
    "    decoder_length=5,\n",
    "    num_units=6,\n",
    "    num_layers=2,\n",
    "    vocab_size=9,\n",
    "    embedding_size=8,\n",
    "    learning_rate=0.01,\n",
    "    max_gradient_norm=5.0,\n",
    "    beam_width=2,\n",
    "    use_attention=False,\n",
    "    num_train_steps=100,\n",
    "    debug_verbose=False,\n",
    "    train_source_swapped=False,\n",
    "    model_folder_in_drive = GoogleDriveFolder.seq2seq.value    \n",
    ")\n",
    "\n",
    "test_attention_hparams = copy.deepcopy(test_hparams)\n",
    "test_attention_hparams.use_attention = True\n",
    "\n",
    "real_hparams = tf.contrib.training.HParams(\n",
    "    batch_size=25,  # of tweets should be dividable by batch_size\n",
    "    encoder_length=20,\n",
    "    decoder_length=20,\n",
    "    num_layers=2,\n",
    "    num_units=1024,\n",
    "    vocab_size=500,\n",
    "    embedding_size=256,\n",
    "    learning_rate=0.01,\n",
    "    max_gradient_norm=5.0,\n",
    "    beam_width=9,\n",
    "    use_attention=False,\n",
    "    num_train_steps=16,\n",
    "    debug_verbose=False,\n",
    "    train_source_swapped=False,\n",
    "    model_folder_in_drive = GoogleDriveFolder.seq2seq.value    \n",
    ")\n",
    "\n",
    "real_swapped_hparams = copy.deepcopy(real_hparams)\n",
    "real_swapped_hparams.train_source_swapped = True\n",
    "\n",
    "large_hparams = tf.contrib.training.HParams(\n",
    "    batch_size=300,  # of tweets should be dividable by batch_size\n",
    "    encoder_length=30,\n",
    "    decoder_length=30,\n",
    "    num_units=1024,\n",
    "    num_layers = 3,\n",
    "    vocab_size=50000,\n",
    "    embedding_size=1024,\n",
    "    learning_rate=0.01,\n",
    "    max_gradient_norm=5.0,\n",
    "    beam_width=2,  # for faster iteration, this should be 10\n",
    "    use_attention=False,\n",
    "    num_train_steps=1000000,\n",
    "    debug_verbose=False,\n",
    "    train_source_swapped=False,\n",
    "    model_folder_in_drive = GoogleDriveFolder.seq2seq.value\n",
    ")\n",
    "\n",
    "large_swapped_hparams = copy.deepcopy(large_hparams)\n",
    "large_swapped_hparams.train_source_swapped = True\n",
    "large_swapped_hparams.model_folder_in_drive = GoogleDriveFolder.seq2seq_swapped.value\n",
    "\n",
    "large_rl_hparams = copy.deepcopy(large_hparams)\n",
    "large_rl_hparams.model_folder_in_drive = GoogleDriveFolder.seq2seq_rl.value\n",
    "\n",
    "# Model path\n",
    "model_path = \"./saved_model/twitter\"\n",
    "\n",
    "# Symbol for start decode process.\n",
    "tgt_sos_id = 0\n",
    "\n",
    "# Symbol for end of decode process.\n",
    "tgt_eos_id = 1\n",
    "\n",
    "pad_id = 2\n",
    "\n",
    "unk_id = 3\n",
    "\n",
    "\n",
    "def info(message, hparams):\n",
    "    if hparams.debug_verbose:\n",
    "        print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DFEYKvBwL3Nm"
   },
   "outputs": [],
   "source": [
    "# For debug purpose.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "class ChatbotModel:\n",
    "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
    "        self.sess = sess\n",
    "        # todo remove\n",
    "        self.hparams = hparams\n",
    "\n",
    "        # todo\n",
    "        self.model_path = model_path\n",
    "        self.name = scope\n",
    "\n",
    "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
    "            hparams, scope)\n",
    "        self.decoder_inputs, self.decoder_target_lengths, logits = self._build_decoder(\n",
    "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
    "            encoder_state, encoder_outputs)\n",
    "\n",
    "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
    "        self.target_labels, self.loss, self.global_step, self.train_op = self._build_optimizer(\n",
    "            hparams, logits)\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        self.merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        # Initialize saver after model created\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "\n",
    "    def restore(self):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
    "        if ckpt:\n",
    "            last_model = ckpt.model_checkpoint_path\n",
    "            self.saver.restore(self.sess, last_model)\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Created fresh model.\")\n",
    "            return False\n",
    "\n",
    "    def train(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
    "              decoder_inputs, decoder_target_lengths, reward=1.0):\n",
    "        feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
    "            self.target_labels: target_labels,\n",
    "            self.decoder_inputs: decoder_inputs,\n",
    "            self.decoder_target_lengths: decoder_target_lengths,\n",
    "            # For normal Seq2Seq reward is always 1.\n",
    "            self.reward: reward\n",
    "        }\n",
    "        _, global_step = self.sess.run(\n",
    "            [self.train_op, self.global_step], feed_dict=feed_dict)\n",
    "        return global_step\n",
    "\n",
    "    def batch_loss(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
    "                   decoder_inputs, decoder_target_lengths):\n",
    "        feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
    "            self.target_labels: target_labels,\n",
    "            self.decoder_inputs: decoder_inputs,\n",
    "            self.decoder_target_lengths: decoder_target_lengths,\n",
    "            # For normal Seq2Seq reward is always 1.\n",
    "            self.reward: 1\n",
    "        }\n",
    "        return self.sess.run([self.loss, self.merged_summary],\n",
    "                             feed_dict=feed_dict)\n",
    "\n",
    "    def train_with_reward(self, infer_model, standard_seq2seq_model,\n",
    "                          encoder_inputs, encoder_inputs_lengths, target_labels,\n",
    "                          decoder_inputs, decoder_target_lengths,\n",
    "                          dull_responses):\n",
    "        infered_replies = infer_model.infer(encoder_inputs,\n",
    "                                            encoder_inputs_lengths)\n",
    "        standard_seq2seq_encoder_inputs = []\n",
    "        standard_seq2seq_encoder_inputs_lengths = []\n",
    "        for reply in infered_replies:\n",
    "            standard_seq2seq_encoder_inputs_lengths.append(len(reply))\n",
    "            if len(reply) <= self.hparams.encoder_length:\n",
    "                standard_seq2seq_encoder_inputs.append(np.append(reply, (\n",
    "                        [pad_id] * (self.hparams.encoder_length - len(reply)))))\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"Inferred\"\n",
    "                    \" reply shouldn't be longer than encoder_input\")\n",
    "        standard_seq2seq_encoder_inputs = np.transpose(\n",
    "            np.array(standard_seq2seq_encoder_inputs))\n",
    "        reward1 = standard_seq2seq_model.reward_ease_of_answering(\n",
    "            standard_seq2seq_encoder_inputs,\n",
    "            standard_seq2seq_encoder_inputs_lengths, dull_responses)\n",
    "        reward2 = 0  # todo\n",
    "        reward3 = 0  # todo\n",
    "        reward = 0.25 * reward1 + 0.25 * reward2 + 0.5 * reward3\n",
    "        return self.train(encoder_inputs, encoder_inputs_lengths, target_labels,\n",
    "                          decoder_inputs, decoder_target_lengths, reward)\n",
    "\n",
    "    def save(self, model_path=None):\n",
    "        if model_path is None:\n",
    "            model_path = self.model_path\n",
    "        model_dir = \"{}/{}\".format(model_path, self.name)\n",
    "        self.saver.save(self.sess, model_dir, global_step=self.global_step)\n",
    "\n",
    "    @staticmethod\n",
    "    def _softmax(x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    def _build_optimizer(self, hparams, logits):\n",
    "        # Target labels\n",
    "        #   As described in doc for sparse_softmax_cross_entropy_with_logits,\n",
    "        #   labels should be [batch_size, decoder_target_lengths]\n",
    "        #   instead of [batch_size, decoder_target_lengths, vocab_size].\n",
    "        #   So labels should have indices instead of vocab_size classes.\n",
    "        target_labels = tf.placeholder(tf.int32, shape=(\n",
    "            hparams.batch_size, hparams.decoder_length), name=\"target_labels\")\n",
    "\n",
    "        # Loss\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=target_labels, logits=logits)\n",
    "\n",
    "        loss = tf.reduce_sum(crossent / tf.to_float(hparams.batch_size))\n",
    "        # Adjust loss with reward.\n",
    "        loss = tf.multiply(loss, self.reward)\n",
    "\n",
    "        # Train\n",
    "        global_step = tf.get_variable(name=\"global_step\", shape=[],\n",
    "                                      dtype=tf.int32,\n",
    "                                      initializer=tf.constant_initializer(0),\n",
    "                                      trainable=False)\n",
    "\n",
    "        # Calculate and clip gradients\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "            gradients, hparams.max_gradient_norm)\n",
    "\n",
    "        # Optimization\n",
    "        optimizer = tf.train.AdamOptimizer(hparams.learning_rate)\n",
    "        device = '/cpu:0'\n",
    "        if has_gpu0():\n",
    "            device = '/gpu:0'\n",
    "            print(\"!!!GPU ENABLED !!!\")\n",
    "        with tf.device(device):\n",
    "            train_op = optimizer.apply_gradients(\n",
    "                zip(clipped_gradients, params), global_step=global_step)\n",
    "        return target_labels, loss, global_step, train_op\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_encoder(hparams, scope):\n",
    "        # Encoder\n",
    "        #   encoder_inputs: [encoder_length, batch_size]\n",
    "        #   This is time major where encoder_length comes\n",
    "        #   first instead of batch_size.\n",
    "        #   encoder_inputs_lengths: [batch_size]\n",
    "        encoder_inputs = tf.placeholder(tf.int32, shape=(\n",
    "            hparams.encoder_length, hparams.batch_size), name=\"encoder_inputs\")\n",
    "        encoder_inputs_lengths = tf.placeholder(tf.int32,\n",
    "                                                shape=hparams.batch_size,\n",
    "                                                name=\"encoder_inputs_lengtsh\")\n",
    "\n",
    "        # Embedding\n",
    "        #   We originally didn't share embedding between encoder and decoder.\n",
    "        #   But now we share it. It makes much easier to calculate rewards.\n",
    "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
    "        #   Should be shared between training and inference.\n",
    "        with tf.variable_scope(scope):\n",
    "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
    "                                                [hparams.vocab_size,\n",
    "                                                 hparams.embedding_size])\n",
    "\n",
    "        # Look up embedding:\n",
    "        #   encoder_inputs: [encoder_length, batch_size]\n",
    "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
    "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
    "                                                    encoder_inputs)\n",
    "\n",
    "        # LSTM cell.\n",
    "        with tf.variable_scope(scope):\n",
    "            # Should be shared between training and inference.\n",
    "            cell_list = []\n",
    "            for _ in range(hparams.num_layers):\n",
    "               cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
    "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "\n",
    "        # Run Dynamic RNN\n",
    "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
    "        #   encoder_state: [batch_size, num_units],\n",
    "        #   this is final state of the cell for each batch.\n",
    "        with tf.variable_scope(scope):\n",
    "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
    "                                                               encoder_emb_inputs,\n",
    "                                                               time_major=True,\n",
    "                                                               dtype=tf.float32,\n",
    "                                                               sequence_length=encoder_inputs_lengths)\n",
    "\n",
    "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_training_decoder(hparams, encoder_inputs_lengths,\n",
    "                                encoder_state, encoder_outputs, decoder_cell,\n",
    "                                decoder_emb_inputs, decoder_target_lengths,\n",
    "                                projection_layer):\n",
    "        # Decoder with helper:\n",
    "        #   decoder_emb_inputs: [decoder_length, batch_size, embedding_size]\n",
    "        #   decoder_target_lengths: [batch_size] vector,\n",
    "        #   which represents each target sequence length.\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inputs,\n",
    "                                                            decoder_target_lengths,\n",
    "                                                            time_major=True)\n",
    "\n",
    "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
    "        if hparams.use_attention:\n",
    "            # Attention\n",
    "            # encoder_outputs is time major, so transopse it to batch major.\n",
    "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
    "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "            # Create an attention mechanism\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                hparams.num_units,\n",
    "                attention_encoder_outputs,\n",
    "                memory_sequence_length=encoder_inputs_lengths)\n",
    "\n",
    "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                decoder_cell, attention_mechanism,\n",
    "                attention_layer_size=hparams.num_units)\n",
    "\n",
    "            initial_state = wrapped_decoder_cell.zero_state(hparams.batch_size,\n",
    "                                                            tf.float32).clone(\n",
    "                cell_state=encoder_state)\n",
    "        else:\n",
    "            wrapped_decoder_cell = decoder_cell\n",
    "            initial_state = encoder_state\n",
    "\n",
    "            # Decoder and decode\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            wrapped_decoder_cell, training_helper, initial_state,\n",
    "            output_layer=projection_layer)\n",
    "\n",
    "        # Dynamic decoding\n",
    "        #   final_outputs.rnn_output: [batch_size, decoder_length,\n",
    "        #                             vocab_size], list of RNN state.\n",
    "        #   final_outputs.sample_id: [batch_size, decoder_length],\n",
    "        #                            list of argmax of rnn_output.\n",
    "        #   final_state: [batch_size, num_units],\n",
    "        #                list of final state of RNN on decode process.\n",
    "        #   final_sequence_lengths: [batch_size], list of each decoded sequence. \n",
    "        final_outputs, _final_state, _final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "            training_decoder)\n",
    "\n",
    "        if hparams.debug_verbose:\n",
    "            print(\"rnn_output.shape=\", final_outputs.rnn_output.shape)\n",
    "            print(\"sample_id.shape=\", final_outputs.sample_id.shape)\n",
    "            print(\"final_state=\", _final_state)\n",
    "            print(\"final_sequence_lengths.shape=\",\n",
    "                  _final_sequence_lengths.shape)\n",
    "\n",
    "        logits = final_outputs.rnn_output\n",
    "        return logits, wrapped_decoder_cell, initial_state\n",
    "\n",
    "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
    "                       encoder_state, encoder_outputs):\n",
    "        # Decoder input\n",
    "        #   decoder_inputs: [decoder_length, batch_size]\n",
    "        #   decoder_target_lengths: [batch_size]\n",
    "        #   This is grand truth target inputs for training.\n",
    "        decoder_inputs = tf.placeholder(tf.int32, shape=(\n",
    "            hparams.decoder_length, hparams.batch_size), name=\"decoder_inputs\")\n",
    "        decoder_target_lengths = tf.placeholder(tf.int32,\n",
    "                                                shape=hparams.batch_size,\n",
    "                                                name=\"decoder_target_lengths\")\n",
    "\n",
    "        # Look up embedding:\n",
    "        #   decoder_inputs: [decoder_length, batch_size]\n",
    "        #   decoder_emb_inp: [decoder_length, batch_size, embedding_size]\n",
    "        decoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
    "                                                    decoder_inputs)\n",
    "\n",
    "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
    "        # Internally, a neural network operates on dense vectors of some size,\n",
    "        # often 256, 512 or 1024 floats (let's say 512 for here). \n",
    "        # But at the end it needs to predict a word\n",
    "        # from the vocabulary which is often much larger,\n",
    "        # e.g., 40000 words. Output projection is the final linear layer\n",
    "        # that converts (projects) from the internal representation\n",
    "        #  to the larger one.\n",
    "        # So, for example, it can consist of a 512 x 40000 parameter matrix\n",
    "        # and a 40000 parameter for the bias vector.\n",
    "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
    "\n",
    "        # We share this between training and inference.\n",
    "        cell_list = []\n",
    "        for _ in range(hparams.num_layers):\n",
    "           cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        \n",
    "#        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
    "\n",
    "        # Training graph\n",
    "        logits, wrapped_decoder_cell, initial_state = self._build_training_decoder(\n",
    "            hparams, encoder_inputs_lengths, encoder_state, encoder_outputs,\n",
    "            decoder_cell, decoder_emb_inputs, decoder_target_lengths,\n",
    "            projection_layer)\n",
    "\n",
    "        return decoder_inputs, decoder_target_lengths, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JzDknaQZV-iU"
   },
   "outputs": [],
   "source": [
    "class ChatbotInferenceModel:\n",
    "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
    "        self.sess = sess\n",
    "        # todo remove\n",
    "        self.hparams = hparams\n",
    "\n",
    "        # todo\n",
    "        self.model_path = model_path\n",
    "        self.name = scope\n",
    "\n",
    "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
    "            hparams, scope)\n",
    "        self.decoder_inputs, self.decoder_target_lengths, self.replies, self.beam_replies, self.infer_logits = self._build_decoder(\n",
    "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
    "            encoder_state, encoder_outputs)\n",
    "\n",
    "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
    "\n",
    "        # we can't use variable length here, \n",
    "        # because tiled_batch requires constant length.\n",
    "        self.batch_size = 1\n",
    "\n",
    "        # Initialize saver after model created\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "\n",
    "    def restore(self):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
    "        if ckpt:\n",
    "            last_model = ckpt.model_checkpoint_path\n",
    "            self.saver.restore(self.sess, last_model)\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Created fresh model.\")\n",
    "            return False\n",
    "\n",
    "    def infer(self, encoder_inputs, encoder_inputs_lengths):\n",
    "        inference_feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
    "        }\n",
    "        replies = self.sess.run(self.replies, feed_dict=inference_feed_dict)\n",
    "        return replies\n",
    "\n",
    "    def infer_beam_search(self, encoder_inputs, encoder_inputs_lengths):\n",
    "        inference_feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
    "        }\n",
    "        replies = self.sess.run(self.beam_replies,\n",
    "                                feed_dict=inference_feed_dict)\n",
    "        return replies\n",
    "     \n",
    "    # imakoko\n",
    "    def infer_mi(self, swapped_model, encoder_inputs, encoder_inputs_lengths):\n",
    "        inference_feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
    "        }\n",
    "        beam_replies = self.sess.run(self.beam_replies, feed_dict=inference_feed_dict)\n",
    "        # beam_replis [batch_size, length, , batch_width]\n",
    "        # for now we assume encoder_inputs is batch_size = 1\n",
    "        \n",
    "        swapped_encoder_inputs = beam_replies[0]\n",
    "        # beam_width = batch_size\n",
    "        swapped_batch_size = swapped_encoder_inputs.shape[1]\n",
    "        \n",
    "        # beam_replies can be shorten less than decoder_output_legth, so we pad them.\n",
    "        paddings = tf.constant([[0, self.hparams.encoder_length - swapped_encoder_inputs.shape[0],], [0, 0]])\n",
    "        swapped_encoder_inputs = swapped_model.sess.run(tf.pad(swapped_encoder_inputs, paddings, \"CONSTANT\", constant_values=pad_id))\n",
    "        swapped_encoder_inputs_lengths = np.empty(swapped_batch_size, dtype=np.int)\n",
    "        for i in range(swapped_batch_size):\n",
    "          swapped_encoder_inputs_lengths[i] = swapped_encoder_inputs.shape[0]\n",
    "        \n",
    "        print(swapped_encoder_inputs.shape)\n",
    "        return swapped_model.infer_beam_search(swapped_encoder_inputs, swapped_encoder_inputs_lengths)\n",
    "        # todo make correct length\n",
    "#        for repy in beam_replies:\n",
    "          # logits from swapped_model for this reply\n",
    "          # cals prob for in original encoder_input\n",
    "      \n",
    "\n",
    "    def log_prob(self, encoder_inputs, encoder_inputs_lengths, expected_output):\n",
    "        \"\"\"Return sum of log probability of given\n",
    "           one specific expected_output for encoder_inputs.\n",
    "    \n",
    "        Args:\n",
    "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
    "            expected_output: [1, decoder_length or less than decoder_length],\n",
    "            eg) One reply.\n",
    "    \n",
    "        Returns:\n",
    "            Return log probablity of expected output for given encoder inputs.\n",
    "            eg) sum of log probability of reply \"Good\" when given [\"How are you?\",\n",
    "             \"What's up?\"]\n",
    "        \"\"\"\n",
    "        inference_feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
    "        }\n",
    "\n",
    "        # Logits\n",
    "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
    "        logits_batch_value = self.sess.run(self.infer_logits,\n",
    "                                           feed_dict=inference_feed_dict)\n",
    "\n",
    "        sum_p = []\n",
    "        # For each batch: [actual_decoder_length, vocab_size]\n",
    "        for logits in logits_batch_value:\n",
    "            p = 1\n",
    "            # Note that expected_output and logits don't always have\n",
    "            # same length, but zip takes care of the case.\n",
    "            for word_id, logit in zip(expected_output, logits):\n",
    "                # Apply softmax first, see definition of softmax.\n",
    "                norm = (self._softmax(logit))[word_id]\n",
    "                p *= norm\n",
    "            p = np.log(p)\n",
    "            sum_p.append(p)\n",
    "        ret = np.sum(sum_p) / len(sum_p)\n",
    "        return ret\n",
    "\n",
    "    def reward_ease_of_answering(self, encoder_inputs, encoder_inputs_lengths,\n",
    "                                 expected_outputs):\n",
    "        \"\"\" Return reward for ease of answering. \n",
    "            See Deep Reinforcement Learning for Dialogue Generation\n",
    "            for more details.\n",
    "    \n",
    "        Args:\n",
    "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
    "            expected_outputs: [number of pre-defined dull responses,\n",
    "            decoder_length or less than decoder_length].\n",
    "            eg) [[\"I'm\", \"Good\"], [\"fine\"]]\n",
    "    \n",
    "        Returns:\n",
    "            Return reward for ease of answering.\n",
    "            Note that this can be calculated\n",
    "            by calling log_prob function for each dull response,\n",
    "            but this function is more efficient\n",
    "            because this calculated the reward at once.\n",
    "        \"\"\"\n",
    "        inference_feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
    "        }\n",
    "\n",
    "        # Logits\n",
    "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
    "        logits_batch_value = self.sess.run(self.infer_logits,\n",
    "                                           feed_dict=inference_feed_dict)\n",
    "\n",
    "        batch_sum_p = []\n",
    "        # For each batch: [actual_decoder_length, vocab_size]\n",
    "        for logits in logits_batch_value:\n",
    "            sum_p = []\n",
    "            for expected_output in expected_outputs:\n",
    "                p = 1\n",
    "                # Note that expected_output and logits don't\n",
    "                # always have same length, but zip takes care of the case.\n",
    "                for word_id, logit in zip(expected_output, logits):\n",
    "                    # Apply softmax first, see definition of softmax.\n",
    "                    norm = (self._softmax(logit))[word_id]\n",
    "                    p *= norm\n",
    "                p = np.log(p) / len(expected_output)\n",
    "                sum_p.append(p)\n",
    "            one_batch_p = np.sum(sum_p)\n",
    "            batch_sum_p.append(one_batch_p)\n",
    "        ret = np.sum(batch_sum_p) / len(batch_sum_p)\n",
    "        return -ret\n",
    "\n",
    "    @staticmethod\n",
    "    def _softmax(x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_encoder(hparams, scope):\n",
    "        # Encoder\n",
    "        #   encoder_inputs: [encoder_length, batch_size]\n",
    "        #   This is time major where encoder_length\n",
    "        #   comes first instead of batch_size.\n",
    "        #   encoder_inputs_lengths: [batch_size]\n",
    "        encoder_inputs = tf.placeholder(tf.int32,\n",
    "                                        shape=[hparams.encoder_length, None],\n",
    "                                        name=\"encoder_inputs\")\n",
    "        encoder_inputs_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"encoder_inputs_lengths\")\n",
    "\n",
    "        # Embedding\n",
    "        #   We originally didn't share embedding between encoder and decoder.\n",
    "        #   But now we share it. It makes much easier to calculate rewards.\n",
    "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
    "        #   Should be shared between training and inference.\n",
    "        with tf.variable_scope(scope):\n",
    "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
    "                                                [hparams.vocab_size,\n",
    "                                                 hparams.embedding_size])\n",
    "\n",
    "        # Look up embedding:\n",
    "        #   encoder_inputs: [encoder_length, batch_size]\n",
    "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
    "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
    "                                                    encoder_inputs)\n",
    "\n",
    "        # LSTM cell.\n",
    "        with tf.variable_scope(scope):\n",
    "            # Should be shared between training and inference.\n",
    "#            encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
    "            cell_list = []\n",
    "            for _ in range(hparams.num_layers):\n",
    "               cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
    "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "\n",
    "        # Run Dynamic RNN\n",
    "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
    "        #   encoder_state: [batch_size, num_units],\n",
    "        #   this is final state of the cell for each batch.\n",
    "        with tf.variable_scope(scope):\n",
    "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
    "                                                               encoder_emb_inputs,\n",
    "                                                               time_major=True,\n",
    "                                                               dtype=tf.float32,\n",
    "                                                               sequence_length=encoder_inputs_lengths)\n",
    "\n",
    "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_greedy_inference(hparams, embedding_encoder, encoder_state,\n",
    "                                encoder_inputs_lengths, encoder_outputs,\n",
    "                                decoder_cell, projection_layer):\n",
    "        # Greedy decoder\n",
    "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding_encoder,\n",
    "            tf.fill([dynamic_batch_size], tgt_sos_id), tgt_eos_id)\n",
    "\n",
    "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
    "        if hparams.use_attention:\n",
    "            # Attention\n",
    "            # encoder_outputs is time major, so transopse it to batch major.\n",
    "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
    "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "            # Create an attention mechanism\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                hparams.num_units,\n",
    "                attention_encoder_outputs,\n",
    "                memory_sequence_length=encoder_inputs_lengths)\n",
    "\n",
    "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                decoder_cell, attention_mechanism,\n",
    "                attention_layer_size=hparams.num_units)\n",
    "\n",
    "            initial_state = wrapped_decoder_cell.zero_state(dynamic_batch_size,\n",
    "                                                            tf.float32).clone(\n",
    "                cell_state=encoder_state)\n",
    "        else:\n",
    "            wrapped_decoder_cell = decoder_cell\n",
    "            initial_state = encoder_state\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            wrapped_decoder_cell, inference_helper, initial_state,\n",
    "            output_layer=projection_layer)\n",
    "\n",
    "        # len(inferred_reply) is lte encoder_length,\n",
    "        # because we are targeting tweet (140 for each tweet)\n",
    "        # Also by doing this,\n",
    "        # we can pass the reply to other seq2seq w/o shorten it.\n",
    "        maximum_iterations = hparams.encoder_length\n",
    "\n",
    "        # Dynamic decoding\n",
    "        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            inference_decoder, maximum_iterations=maximum_iterations)\n",
    "        replies = outputs.sample_id\n",
    "\n",
    "        # We use infer_logits instead of logits when calculating log_prob,\n",
    "        # because infer_logits doesn't require decoder_target_lengths input.\n",
    "        infer_logits = outputs.rnn_output\n",
    "        return infer_logits, replies\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_beam_search_inference(hparams, encoder_inputs_lengths,\n",
    "                                     embedding_encoder, encoder_state,\n",
    "                                     encoder_outputs, decoder_cell,\n",
    "                                     projection_layer):\n",
    "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
    "        # https://github.com/tensorflow/tensorflow/issues/11904\n",
    "        if hparams.use_attention:\n",
    "            # Attention\n",
    "            # encoder_outputs is time major, so transopse it to batch major.\n",
    "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
    "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "            tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
    "                attention_encoder_outputs, multiplier=hparams.beam_width)\n",
    "            tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(\n",
    "                encoder_state, multiplier=hparams.beam_width)\n",
    "            tiled_encoder_inputs_lengths = tf.contrib.seq2seq.tile_batch(\n",
    "                encoder_inputs_lengths, multiplier=hparams.beam_width)\n",
    "\n",
    "            # Create an attention mechanism\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                hparams.num_units, tiled_encoder_outputs,\n",
    "                memory_sequence_length=tiled_encoder_inputs_lengths)\n",
    "\n",
    "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                decoder_cell, attention_mechanism,\n",
    "                attention_layer_size=hparams.num_units)\n",
    "\n",
    "            decoder_initial_state = wrapped_decoder_cell.zero_state(\n",
    "                dtype=tf.float32,\n",
    "                batch_size=dynamic_batch_size * hparams.beam_width)\n",
    "            decoder_initial_state = decoder_initial_state.clone(\n",
    "                cell_state=tiled_encoder_final_state)\n",
    "        else:\n",
    "            wrapped_decoder_cell = decoder_cell\n",
    "            decoder_initial_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n",
    "                                                                  multiplier=hparams.beam_width)\n",
    "\n",
    "        # len(inferred_reply) is lte encoder_length,\n",
    "        # because we are targeting tweet (140 for each tweet)\n",
    "        # Also by doing this,\n",
    "        # we can pass the reply to other seq2seq w/o shorten it.\n",
    "        maximum_iterations = hparams.encoder_length\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "            cell=wrapped_decoder_cell,\n",
    "            embedding=embedding_encoder,\n",
    "            start_tokens=tf.fill([dynamic_batch_size], tgt_sos_id),\n",
    "            end_token=tgt_eos_id,\n",
    "            initial_state=decoder_initial_state,\n",
    "            beam_width=hparams.beam_width,\n",
    "            output_layer=projection_layer,\n",
    "            length_penalty_weight=0.0)\n",
    "\n",
    "        # Dynamic decoding\n",
    "        beam_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            inference_decoder, maximum_iterations=maximum_iterations)\n",
    "        beam_replies = beam_outputs.predicted_ids\n",
    "        return beam_replies\n",
    "\n",
    "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
    "                       encoder_state, encoder_outputs):\n",
    "        # Decoder input\n",
    "        #   decoder_inputs: [decoder_length, batch_size]\n",
    "        #   decoder_target_lengths: [batch_size]\n",
    "        #   This is grand truth target inputs for training.\n",
    "        decoder_inputs = tf.placeholder(tf.int32,\n",
    "                                        shape=[hparams.decoder_length, None],\n",
    "                                        name=\"decoder_inputs\")\n",
    "        decoder_target_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"decoder_target_lengths\")\n",
    "\n",
    "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
    "        # Internally, a neural network operates on dense vectors of some size,\n",
    "        # often 256, 512 or 1024 floats (let's say 512 for here). \n",
    "        # But at the end it needs to predict a word\n",
    "        # from the vocabulary which is often much larger,\n",
    "        # e.g., 40000 words. Output projection is the final linear\n",
    "        # layer that converts (projects) from the internal\n",
    "        # representation to the larger one.\n",
    "        # So, for example, it can consist of a 512 x 40000 parameter\n",
    "        # matrix and a 40000 parameter for the bias vector.\n",
    "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
    "\n",
    "        # We share this between training and inference.\n",
    "#        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
    "        cell_list = []\n",
    "        for _ in range(hparams.num_layers):\n",
    "           cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        \n",
    "\n",
    "        # Greedy Inference graph\n",
    "        infer_logits, replies = self._build_greedy_inference(hparams,\n",
    "                                                             embedding_encoder,\n",
    "                                                             encoder_state,\n",
    "                                                             encoder_inputs_lengths,\n",
    "                                                             encoder_outputs,\n",
    "                                                             decoder_cell,\n",
    "                                                             projection_layer)\n",
    "\n",
    "        # Beam Search Inference graph\n",
    "        beam_replies = self._build_beam_search_inference(hparams,\n",
    "                                                         encoder_inputs_lengths,\n",
    "                                                         embedding_encoder,\n",
    "                                                         encoder_state,\n",
    "                                                         encoder_outputs,\n",
    "                                                         decoder_cell,\n",
    "                                                         projection_layer)\n",
    "\n",
    "        return decoder_inputs, decoder_target_lengths, replies, beam_replies, infer_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceHelper:\n",
    "    def __init__(self, model, vocab, rev_vocab):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.rev_vocab = rev_vocab\n",
    "\n",
    "    def inferences(self, tweet):\n",
    "        ret = []\n",
    "        # todo move create_inference+inout\n",
    "        encoder_inputs, encoder_inputs_lengths = create_inference_input(tweet,\n",
    "                                                                         self.model.hparams,\n",
    "                                                                         self.vocab)\n",
    "        replies = self.model.infer(encoder_inputs, encoder_inputs_lengths)\n",
    "        ids = replies[0].tolist()\n",
    "        # todo ids_to_words\n",
    "        reply = ids_to_words(ids, self.rev_vocab)\n",
    "        ret.append(reply)\n",
    "        beam_replies = self.model.infer_beam_search(encoder_inputs,\n",
    "                                                    encoder_inputs_lengths)\n",
    "\n",
    "        ret.extend([ids_to_words(beam_replies[0][:, i], self.rev_vocab) for i in\n",
    "                    range(self.model.hparams.beam_width)])\n",
    "        return ret\n",
    "\n",
    "    def print_inferences(self, tweet):\n",
    "        print(tweet)\n",
    "        for i, reply in enumerate(self.inferences(tweet)):\n",
    "            print(\"    {}{}\".format(\"beam:\" if i != 0 else \"\", reply))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kK5zkPkRkUqa"
   },
   "outputs": [],
   "source": [
    "def clear_saved_model():\n",
    "    # don't remove thisSS\n",
    "#    shutil.rmtree(p(\"saved_model\"))\n",
    "    os.makedirs(p(\"saved_model\"), exist_ok=True)\n",
    "    shutil.rmtree(p(\"saved_model2\"))\n",
    "    os.makedirs(p(\"saved_model2\"), exist_ok=True)\n",
    "#    shutil.rmtree(p(\"saved_model3\"))\n",
    "    os.makedirs(p(\"saved_model3\"), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DQg8kU-2Dr-q"
   },
   "outputs": [],
   "source": [
    "# Helper functions to test\n",
    "def make_test_training_data(hparams):\n",
    "    train_encoder_inputs = np.empty(\n",
    "        (hparams.encoder_length, hparams.batch_size), dtype=np.int)\n",
    "    train_encoder_inputs_lengths = np.empty(hparams.batch_size, dtype=np.int)\n",
    "    training_target_labels = np.empty(\n",
    "        (hparams.batch_size, hparams.decoder_length), dtype=np.int)\n",
    "    training_decoder_inputs = np.empty(\n",
    "        (hparams.decoder_length, hparams.batch_size), dtype=np.int)\n",
    "\n",
    "    # We keep first tweet to validate inference.\n",
    "    first_tweet = None\n",
    "\n",
    "    for i in range(hparams.batch_size):\n",
    "        # Tweet\n",
    "        tweet = np.random.randint(low=0, high=hparams.vocab_size,\n",
    "                                  size=hparams.encoder_length)\n",
    "        train_encoder_inputs[:, i] = tweet\n",
    "        train_encoder_inputs_lengths[i] = len(tweet)\n",
    "        # Reply\n",
    "        #   Note that low = 2, as 0 and 1 are reserved.\n",
    "        reply = np.random.randint(low=2, high=hparams.vocab_size,\n",
    "                                  size=hparams.decoder_length - 1)\n",
    "\n",
    "        training_target_label = np.concatenate((reply, np.array([tgt_eos_id])))\n",
    "        training_target_labels[i] = training_target_label\n",
    "\n",
    "        training_decoder_input = np.concatenate(([tgt_sos_id], reply))\n",
    "        training_decoder_inputs[:, i] = training_decoder_input\n",
    "\n",
    "        if i == 0:\n",
    "            first_tweet = tweet\n",
    "            info(\"0th tweet={}\".format(tweet), hparams)\n",
    "            info(\"0th reply_with_eos_suffix={}\".format(training_target_label),\n",
    "                 hparams)\n",
    "            info(\"0th reply_with_sos_prefix={}\".format(training_decoder_input),\n",
    "                 hparams)\n",
    "\n",
    "        info(\"Tweets\", hparams)\n",
    "        info(train_encoder_inputs, hparams)\n",
    "        info(\"Replies\", hparams)\n",
    "        info(training_target_labels, hparams)\n",
    "        info(training_decoder_inputs, hparams)\n",
    "    return first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs\n",
    "\n",
    "\n",
    "def test_training(test_hparams, model, infer_model):\n",
    "    if test_hparams.use_attention:\n",
    "        print(\"==== training model[attention] ====\")\n",
    "    else:\n",
    "        print(\"==== training model ====\")\n",
    "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
    "        test_hparams)\n",
    "    for i in range(test_hparams.num_train_steps):\n",
    "        _ = model.train(train_encoder_inputs,\n",
    "                        train_encoder_inputs_lengths,\n",
    "                        training_target_labels,\n",
    "                        training_decoder_inputs,\n",
    "                        np.ones(test_hparams.batch_size,\n",
    "                                dtype=int) * test_hparams.decoder_length)\n",
    "        if i % 5 == 0 and test_hparams.debug_verbose:\n",
    "            print('.', end='')\n",
    "\n",
    "        if i % 15 == 0:\n",
    "            model.save()\n",
    "\n",
    "    inference_encoder_inputs = np.empty((test_hparams.encoder_length, 1),\n",
    "                                        dtype=np.int)\n",
    "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
    "    for i in range(1):\n",
    "        inference_encoder_inputs[:, i] = first_tweet\n",
    "        inference_encoder_inputs_lengths[i] = len(first_tweet)\n",
    "\n",
    "    # testing \n",
    "    log_prob54 = infer_model.log_prob(inference_encoder_inputs,\n",
    "                                      inference_encoder_inputs_lengths,\n",
    "                                      np.array([5, 4]))\n",
    "    log_prob65 = infer_model.log_prob(inference_encoder_inputs,\n",
    "                                      inference_encoder_inputs_lengths,\n",
    "                                      np.array([6, 5]))\n",
    "    print(\"log_prob for 54\", log_prob54)\n",
    "    print(\"log_prob for 65\", log_prob65)\n",
    "\n",
    "    reward = infer_model.reward_ease_of_answering(inference_encoder_inputs,\n",
    "                                                  inference_encoder_inputs_lengths,\n",
    "                                                  np.array([[5], [6]]))\n",
    "    print(\"reward=\", reward)\n",
    "\n",
    "    if test_hparams.debug_verbose:\n",
    "        print(inference_encoder_inputs)\n",
    "    replies = infer_model.infer(inference_encoder_inputs,\n",
    "                                inference_encoder_inputs_lengths)\n",
    "    print(\"Infered replies\", replies[0])\n",
    "    print(\"Expected replies\", training_target_labels[0])\n",
    "\n",
    "    beam_replies = infer_model.infer_beam_search(inference_encoder_inputs,\n",
    "                                                 inference_encoder_inputs_lengths)\n",
    "    print(\"Infered replies candidate0\", beam_replies[0][:, 0])\n",
    "    print(\"Infered replies candidate1\", beam_replies[0][:, 1])\n",
    "\n",
    "\n",
    "def create_train_infer_models(graph, sess, hparams, model_path,\n",
    "                              force_restore=False):\n",
    "    with graph.as_default():\n",
    "        with tf.variable_scope('root'):\n",
    "            model = ChatbotModel(sess, hparams, model_path=model_path)\n",
    "\n",
    "        with tf.variable_scope('root', reuse=True):\n",
    "            infer_model = ChatbotInferenceModel(sess, hparams,\n",
    "                                                model_path=model_path)\n",
    "            restored = model.restore()\n",
    "            if not restored:\n",
    "                if force_restore:\n",
    "                    raise Exception(\"Oops, couldn't restore\")\n",
    "                else:\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "        return model, infer_model\n",
    "\n",
    "\n",
    "def create_train_infer_models_in_graphs(train_graph, train_sess, infer_graph,\n",
    "                                        infer_sess, hparams, model_path):\n",
    "    with train_graph.as_default():\n",
    "        with tf.variable_scope('root'):\n",
    "            model = ChatbotModel(train_sess, hparams, model_path=model_path)\n",
    "            if not model.restore():\n",
    "                train_sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # note that infer_model is not sharing variable with training model.\n",
    "    with infer_graph.as_default():\n",
    "        with tf.variable_scope('root'):\n",
    "            infer_model = ChatbotInferenceModel(infer_sess, hparams,\n",
    "                                                model_path=model_path)\n",
    "\n",
    "    return model, infer_model\n",
    "\n",
    "\n",
    "def test_multiple_models_training():\n",
    "    first_tweet, train_encoder_inputs, train_encoder_inputs_length, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
    "        test_hparams)\n",
    "\n",
    "    graph1 = tf.Graph()\n",
    "    sess1 = tf.Session(graph=graph1)\n",
    "    model, infer_model = create_train_infer_models(graph1, sess1, test_hparams,\n",
    "                                                   p(\"saved_model/hige\"))\n",
    "    test_training(test_hparams, model, infer_model)\n",
    "\n",
    "    graph2 = tf.Graph()\n",
    "    sess2 = tf.Session(graph=graph2)\n",
    "    model2, infer_model2 = create_train_infer_models(graph2, sess2,\n",
    "                                                     test_hparams,\n",
    "                                                     p(\"saved_model2/hige\"))\n",
    "\n",
    "    test_training(test_hparams, model2, infer_model2)\n",
    "    dull_responses = [[4, 6, 6], [5, 5]]\n",
    "    model2.train_with_reward(infer_model2, infer_model, train_encoder_inputs,\n",
    "                             train_encoder_inputs_length,\n",
    "                             training_target_labels, training_decoder_inputs,\n",
    "                             np.ones((test_hparams.batch_size),\n",
    "                                     dtype=int) * test_hparams.decoder_length,\n",
    "                             dull_responses)\n",
    "\n",
    "    # comment out until https://github.com/tensorflow/tensorflow/issues/10731 is fixed\n",
    "    graph3 = tf.Graph()\n",
    "    sess3 = tf.Session(graph=graph3)\n",
    "#    model3, infer_model3 = create_train_infer_models(graph3, sess3, test_attention_hparams, \"./saved_model3/hige\")    \n",
    "#    test_training(test_attention_hparams, model3, infer_model3)        \n",
    "\n",
    "\n",
    "def test_save_restore_multiple_models_training():\n",
    "    clear_saved_model()\n",
    "\n",
    "    # Fresh model\n",
    "    test_multiple_models_training()\n",
    "\n",
    "    # Saved model\n",
    "    test_multiple_models_training()\n",
    "\n",
    "\n",
    "def test_distributed_pattern(hparams):\n",
    "    clear_saved_model()\n",
    "\n",
    "    if hparams.use_attention:\n",
    "        print(\"==== test_distributed_pattern[attention] ====\")\n",
    "    else:\n",
    "        print(\"==== test_distributed_pattern ====\")\n",
    "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
    "        hparams)\n",
    "\n",
    "    train_graph = tf.Graph()\n",
    "    infer_graph = tf.Graph()\n",
    "    train_sess = tf.Session(graph=train_graph)\n",
    "    infer_sess = tf.Session(graph=infer_graph)\n",
    "\n",
    "    model, infer_model = create_train_infer_models_in_graphs(train_graph,\n",
    "                                                             train_sess,\n",
    "                                                             infer_graph,\n",
    "                                                             infer_sess,\n",
    "                                                             hparams,\n",
    "                                                             p(\n",
    "                                                                 \"saved_model/hige\"))\n",
    "\n",
    "    for i in range(hparams.num_train_steps):\n",
    "        _ = model.train(train_encoder_inputs,\n",
    "                        train_encoder_inputs_lengths,\n",
    "                        training_target_labels,\n",
    "                        training_decoder_inputs,\n",
    "                        np.ones(hparams.batch_size,\n",
    "                                dtype=int) * hparams.decoder_length)\n",
    "\n",
    "    model.save()\n",
    "\n",
    "    inference_encoder_inputs = np.empty((hparams.encoder_length, 1),\n",
    "                                        dtype=np.int)\n",
    "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
    "\n",
    "    inference_encoder_inputs[:, 0] = first_tweet\n",
    "    inference_encoder_inputs_lengths[0] = len(first_tweet)\n",
    "\n",
    "    infer_model.restore()\n",
    "    replies = infer_model.infer(inference_encoder_inputs,\n",
    "                                inference_encoder_inputs_lengths)\n",
    "    print(\"Inferred replies\", replies[0])\n",
    "    print(\"Expected replies\", training_target_labels[0])\n",
    "\n",
    "    beam_replies = infer_model.infer_beam_search(inference_encoder_inputs,\n",
    "                                                 inference_encoder_inputs_lengths)\n",
    "    print(\"Inferred replies candidate0\", beam_replies[0][:, 0])\n",
    "    print(\"Inferred replies candidate1\", beam_replies[0][:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "W_ciBCflZq5o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fresh model.\n",
      "==== training model ====\n",
      "log_prob for 54 -5.48318064474\n",
      "log_prob for 65 -5.31008536416\n",
      "reward= 2.96000471118\n",
      "Infered replies [6 8 2 2 1]\n",
      "Expected replies [6 8 2 2 1]\n",
      "Infered replies candidate0 [6 8 2 2 1]\n",
      "Infered replies candidate1 [6 6 2 2 1]\n",
      "Created fresh model.\n",
      "==== training model ====\n",
      "log_prob for 54 -14.5563873893\n",
      "log_prob for 65 -7.68653722052\n",
      "reward= 7.74087187485\n",
      "Infered replies [6 8 7 2 1]\n",
      "Expected replies [6 8 7 2 1]\n",
      "Infered replies candidate0 [6 8 7 2 1]\n",
      "Infered replies candidate1 [ 6  8  7  1 -1]\n",
      "INFO:tensorflow:Restoring parameters from /Users/higepon/chatbot_data/saved_model/hige/ChatbotModel-91\n",
      "==== training model ====\n",
      "log_prob for 54 -8.65105322702\n",
      "log_prob for 65 -7.64508431457\n",
      "reward= 7.90647441934\n",
      "Infered replies [7 2 3 5 1]\n",
      "Expected replies [7 2 3 5 1]\n",
      "Infered replies candidate0 [7 2 3 5 1]\n",
      "Infered replies candidate1 [7 2 3 7 1]\n",
      "INFO:tensorflow:Restoring parameters from /Users/higepon/chatbot_data/saved_model2/hige/ChatbotModel-91\n",
      "==== training model ====\n",
      "log_prob for 54 -13.2739393146\n",
      "log_prob for 65 -9.85507384013\n",
      "reward= 7.97754060645\n",
      "Infered replies [7 6 7 8 1]\n",
      "Expected replies [2 6 7 8 1]\n",
      "Infered replies candidate0 [2 6 7 8 1]\n",
      "Infered replies candidate1 [7 6 7 8 1]\n",
      "==== test_distributed_pattern ====\n",
      "Created fresh model.\n",
      "INFO:tensorflow:Restoring parameters from /Users/higepon/chatbot_data/saved_model/hige/ChatbotModel-100\n",
      "Inferred replies [7 7 7 1]\n",
      "Expected replies [7 7 8 7 1]\n",
      "Inferred replies candidate0 [ 7  7  7  1 -1]\n",
      "Inferred replies candidate1 [7 7 8 7 1]\n"
     ]
    }
   ],
   "source": [
    "test_save_restore_multiple_models_training()\n",
    "\n",
    "test_distributed_pattern(test_hparams)\n",
    "\n",
    "# todo\n",
    "# test_distributed_pattern(test_attention_hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kxeWpXO5FThm"
   },
   "outputs": [],
   "source": [
    "def download_file_if_necessary(file_name):\n",
    "    if os.path.exists(file_name):\n",
    "        return\n",
    "    print(\"downloading {}...\".format(file_name))\n",
    "    content = read_file_from_drive(file_name)\n",
    "    f = open(file_name, 'w')\n",
    "    f.write(content)\n",
    "    f.close()\n",
    "    print(\"downloaded\")\n",
    "\n",
    "\n",
    "def read_file_from_drive(file_name):\n",
    "  \n",
    "    seq2seq_data_dir_id = GoogleDriveFolder.root.value\n",
    "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
    "        seq2seq_data_dir_id)}).GetList()\n",
    "    found = [file for file in file_list if file['title'] == file_name]\n",
    "    if found:\n",
    "        downloaded = drive.CreateFile({'id': found[0]['id']})\n",
    "        return downloaded.GetContentString()\n",
    "    else:\n",
    "        raise ValueError(\"file {} not found.\".format(file_name))\n",
    "\n",
    "\n",
    "def read_vocabulary(vocabulary_path):\n",
    "    download_file_if_necessary(vocabulary_path)\n",
    "    rev_vocab = []\n",
    "    rev_vocab.extend(read_file(vocabulary_path).splitlines())\n",
    "    rev_vocab = [line.strip() for line in rev_vocab]\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "    return vocab, rev_vocab\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    f = open(file_name)\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "W3nUhj80H6BE"
   },
   "outputs": [],
   "source": [
    "def download_model_data_if_necessary(drive, model_folder_in_drive, model_path):\n",
    "    if drive is None:\n",
    "        return\n",
    "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
    "        model_folder_in_drive)}).GetList()\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    for file in file_list:\n",
    "        print(\"Downloading \", file['title'], \"...\", end='')\n",
    "        target_file = \"{}/{}\".format(model_path, file['title'])\n",
    "        if not os.path.exists(target_file):\n",
    "            file.GetContentFile(\"{}/{}\".format(model_path, file['title']))\n",
    "        print(\"done\")\n",
    "\n",
    "def generic_train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
    "                       model_path, hparams, generate_models_func,\n",
    "                       inference_hook_func, drive=None, short_loop=False):\n",
    "    def save_and_infer():\n",
    "        model.save()\n",
    "        inference_hook_func(infer_model)\n",
    "        helper = InferenceHelper(infer_model, vocab, rev_vocab)\n",
    "        print(\"==== {} ====\".format(global_step))\n",
    "        helper.print_inferences(\"お疲れ様\")\n",
    "\n",
    "    print(\"generic train loop {}:{}\".format(drive, hparams.model_folder_in_drive))\n",
    "    download_model_data_if_necessary(drive, hparams.model_folder_in_drive, model_path)\n",
    "\n",
    "    graph, sess, model, infer_model = generate_models_func(hparams, model_path)\n",
    "    writer = tf.summary.FileWriter(\"/tmp/validation_loss_log\", graph)\n",
    "\n",
    "    with graph.as_default():\n",
    "        train_data_iterator = train_feed_data.make_one_shot_iterator()\n",
    "        val_data_iterator = val_feed_data.make_one_shot_iterator()\n",
    "\n",
    "        last_saved_time = datetime.datetime.now()\n",
    "        last_time = datetime.datetime.now()\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(hparams.num_train_steps):\n",
    "            train_data = sess.run(train_data_iterator.get_next())\n",
    "            enc_input_index = 0 if not hparams.train_source_swapped else 3\n",
    "            enc_input_length_index = 1 if not hparams.train_source_swapped else 4\n",
    "            dec_input_index = 3 if not hparams.train_source_swapped else 0\n",
    "            dec_input_length_index = 4 if not hparams.train_source_swapped else 1\n",
    "            \n",
    "            global_step = model.train(train_data[enc_input_index], train_data[enc_input_length_index],\n",
    "                                      train_data[2], train_data[dec_input_index],\n",
    "                                      train_data[dec_input_length_index])\n",
    "\n",
    "            if short_loop and i == 2:\n",
    "                save_and_infer()\n",
    "                break\n",
    "            elif i != 0 and i % 15 == 0:\n",
    "                save_and_infer()\n",
    "                val_data = sess.run(val_data_iterator.get_next())\n",
    "                val_loss, val_loss_log = model.batch_loss(val_data[enc_input_index],\n",
    "                                                          val_data[enc_input_length_index],\n",
    "                                                          val_data[2],\n",
    "                                                          val_data[dec_input_index],\n",
    "                                                          val_data[dec_input_length_index])\n",
    "                writer.add_summary(val_loss_log, global_step)\n",
    "                print(\"validation loss\", val_loss)\n",
    "                delta = (\n",
    "                                datetime.datetime.now() - last_time).total_seconds() * 1000\n",
    "                print(\n",
    "                    \"{:.2f} msec/data\".format(delta / hparams.batch_size / 15))\n",
    "                last_time = datetime.datetime.now()\n",
    "                x.append(global_step)\n",
    "                y.append(val_loss)\n",
    "            else:\n",
    "                print('.', end='')\n",
    "            now = datetime.datetime.now()\n",
    "            if (\n",
    "                    now - last_saved_time).total_seconds() > 3600 and drive is not None:\n",
    "                drive = make_drive()\n",
    "                last_saved_time = datetime.datetime.now()\n",
    "                save_model_in_drive(drive, hparams.model_folder_in_drive, model_path)\n",
    "\n",
    "            if i != 0 and i % 100 == 0:\n",
    "                plot_validation_loss(x, y)\n",
    "\n",
    "\n",
    "def plot_validation_loss(x, y):\n",
    "  if colab():\n",
    "    plt.plot(x, y, label=\"Validation Loss\")\n",
    "    plt.plot()\n",
    "    plt.ylabel(\"Validation Loss\")\n",
    "    plt.xlabel(\"steps\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_loop(train_feed_data, val_feed_data, vocab, rev_vocab, model_path,\n",
    "               hparams, drive=None, short_loop=False):\n",
    "    def inference_hook(_):\n",
    "        None\n",
    "\n",
    "    def generate_models(local_hparams, local_model_path):\n",
    "        graph = tf.Graph()\n",
    "        sess = tf.Session(graph=graph)\n",
    "        model, infer_model = create_train_infer_models(graph, sess,\n",
    "                                                       local_hparams,\n",
    "                                                       local_model_path)\n",
    "        return graph, sess, model, infer_model\n",
    "\n",
    "    generic_train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
    "                       model_path, hparams, generate_models, inference_hook,\n",
    "                       drive, short_loop)\n",
    "\n",
    "\n",
    "def train_loop_distributed_pattern(train_feed_data, val_feed_data, vocab,\n",
    "                                   rev_vocab, model_path, hparams, drive=None,\n",
    "                                   short_loop=False):\n",
    "    def inference_hook(infer_model):\n",
    "        # always restore from file, because it's in different graph.\n",
    "        restored = infer_model.restore()\n",
    "        assert restored\n",
    "\n",
    "    def generate_models(local_hparams, local_model_path):\n",
    "        train_graph = tf.Graph()\n",
    "        infer_graph = tf.Graph()\n",
    "        # See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
    "        config = tf.ConfigProto(log_device_placement=True)\n",
    "        config.gpu_options.allow_growth = True\n",
    "        train_sess = tf.Session(graph=train_graph, config=config)\n",
    "        print(\"train_sess=\", train_sess)\n",
    "        infer_sess = tf.Session(graph=infer_graph, config=config)\n",
    "\n",
    "        device = '/cpu:0'\n",
    "        if has_gpu0():\n",
    "            device = '/gpu:0'\n",
    "            print(\"!!!GPU ENABLED !!!\")\n",
    "        with tf.device(device):\n",
    "            model, infer_model = create_train_infer_models_in_graphs(\n",
    "                train_graph,\n",
    "                train_sess,\n",
    "                infer_graph,\n",
    "                infer_sess,\n",
    "                local_hparams,\n",
    "                local_model_path)\n",
    "        return train_graph, train_sess, model, infer_model\n",
    "\n",
    "    generic_train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
    "                       model_path, hparams, generate_models, inference_hook,\n",
    "                       drive, short_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1ouPJTAVZEM6"
   },
   "outputs": [],
   "source": [
    "def train_rl_loop_distributed_pattern(train_feed_data, val_feed_data, vocab,\n",
    "                                      rev_vocab,\n",
    "                                      src_model_path, dst_model_path, hparams,\n",
    "                                      drive=None, short_loop=False):\n",
    "    dull_responses = map(lambda words: words_to_ids(words, vocab),\n",
    "                         [[\"おはよう\"], [\"おつかれ\"]])\n",
    "    print(dull_responses)\n",
    "\n",
    "    download_model_data_if_necessary(drive, hparams.model_folder_in_drive, src_model_path)\n",
    "\n",
    "    seq2seq_graph = tf.Graph()\n",
    "    rl_graph = tf.Graph()\n",
    "\n",
    "    seq2seq_sess = tf.Session(graph=seq2seq_graph)\n",
    "    rl_sess = tf.Session(graph=rl_graph)\n",
    "\n",
    "    with seq2seq_graph.as_default():\n",
    "        with tf.variable_scope('root'):\n",
    "            seq2seq_infer_model = ChatbotInferenceModel(seq2seq_sess, hparams,\n",
    "                                                        model_path=src_model_path)\n",
    "            restored = seq2seq_infer_model.restore()\n",
    "            assert restored\n",
    "\n",
    "    model, infer_model = create_train_infer_models(rl_graph, rl_sess, hparams,\n",
    "                                                   src_model_path,\n",
    "                                                   force_restore=True)\n",
    "    writer = tf.summary.FileWriter(\"/tmp/validation_rl_loss_log\", rl_graph)\n",
    "    with rl_graph.as_default():\n",
    "        train_data_iterator = train_feed_data.make_one_shot_iterator()\n",
    "        val_data_iterator = val_feed_data.make_one_shot_iterator()\n",
    "\n",
    "    last_saved_time = datetime.datetime.now()\n",
    "    last_time = datetime.datetime.now()\n",
    "    x = []\n",
    "    y = []\n",
    "    helper = InferenceHelper(infer_model, vocab, rev_vocab)\n",
    "    for i in range(hparams.num_train_steps):\n",
    "        train_data = rl_sess.run(train_data_iterator.get_next())\n",
    "        global_step = model.train_with_reward(infer_model,\n",
    "                                              seq2seq_infer_model,\n",
    "                                              train_data[0],\n",
    "                                              train_data[1],\n",
    "                                              train_data[2],\n",
    "                                              train_data[3],\n",
    "                                              train_data[4],\n",
    "                                              dull_responses)\n",
    "\n",
    "        if short_loop and i == 2:\n",
    "            model.save()\n",
    "            print(\"==== {} ====\".format(global_step))\n",
    "            helper.print_inferences(\"お疲れ様ー\")            \n",
    "            break\n",
    "        elif i != 0 and i % 15 == 0:\n",
    "            model.save(dst_model_path)\n",
    "            print(\"==== {} ====\".format(global_step))\n",
    "            helper.print_inferences(\"お疲れ様ー\")\n",
    "            val_data = rl_sess.run(val_data_iterator.get_next())\n",
    "            val_loss, val_loss_log = model.batch_loss(val_data[0],\n",
    "                                                      val_data[1],\n",
    "                                                      val_data[2],\n",
    "                                                      val_data[3],\n",
    "                                                      val_data[4])\n",
    "            writer.add_summary(val_loss_log, global_step)\n",
    "            print(\"validation loss\", val_loss)\n",
    "            delta = (datetime.datetime.now() - last_time).total_seconds() * 1000\n",
    "            print(\"{:.2f} msec/data\".format(delta / hparams.batch_size / 15))\n",
    "            last_time = datetime.datetime.now()\n",
    "            x.append(global_step)\n",
    "            y.append(val_loss)\n",
    "\n",
    "            now = datetime.datetime.now()\n",
    "            if (\n",
    "                    now - last_saved_time).total_seconds() > 900 and drive is not None:\n",
    "                save_model_in_drive(drive, hparams.model_folder_in_drive, dst_model_path)\n",
    "                last_saved_time = datetime.datetime.now()\n",
    "\n",
    "            if i != 0 and i % 100 == 0:\n",
    "                plot_validation_loss(x, y)\n",
    "        else:\n",
    "            print('.', end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "k1R4Q230eV-D"
   },
   "outputs": [],
   "source": [
    "def create_encoder_idx_padded(src_file, dst_file, dst_length_file,\n",
    "                              max_line_len):\n",
    "    with open(src_file) as fin, open(dst_file, \"w\") as fout, open(\n",
    "            dst_length_file, \"w\") as flen:\n",
    "        line = fin.readline()\n",
    "        while line:\n",
    "            ids = [int(x) for x in line.split()]\n",
    "            if len(ids) > max_line_len:\n",
    "                ids = ids[:max_line_len]\n",
    "            flen.write(str(len(ids)))\n",
    "            flen.write(\"\\n\")\n",
    "            if len(ids) < max_line_len:\n",
    "                ids.extend([pad_id] * (max_line_len - len(ids)))\n",
    "            ids = [str(x) for x in ids]\n",
    "            fout.write(\" \".join(ids))\n",
    "            fout.write(\"\\n\")\n",
    "            line = fin.readline()\n",
    "\n",
    "\n",
    "# read decoder_idx file and append eos at the end of idx list.\n",
    "def create_decoder_idx_eos(src_file, dst_file, max_line_len):\n",
    "    with open(src_file) as fin, open(dst_file, \"w\") as fout:\n",
    "        line = fin.readline()\n",
    "        while line:\n",
    "            ids = [int(x) for x in line.split()]\n",
    "            if len(ids) > max_line_len - 1:\n",
    "                ids = ids[:max_line_len - 1]\n",
    "            ids.append(tgt_eos_id)\n",
    "            if len(ids) < max_line_len:\n",
    "                ids.extend([pad_id] * (max_line_len - len(ids)))\n",
    "            ids = [str(x) for x in ids]\n",
    "            fout.write(\" \".join(ids))\n",
    "            fout.write(\"\\n\")\n",
    "            line = fin.readline()\n",
    "\n",
    "\n",
    "# read decoder_idx file and put sos at the beginning of the idx list.\n",
    "# also write out length of index list.\n",
    "def create_decoder_idx_sos(src_file, dst_file, dst_length_file, max_line_len):\n",
    "    with open(src_file) as fin, open(dst_file, \"w\") as fout, open(\n",
    "            dst_length_file, \"w\") as flen:\n",
    "        line = fin.readline()\n",
    "        while line:\n",
    "            ids = [tgt_sos_id]\n",
    "            ids.extend([int(x) for x in line.split()])\n",
    "            if len(ids) > max_line_len:\n",
    "                ids = ids[:max_line_len]\n",
    "            flen.write(str(len(ids)))\n",
    "            flen.write(\"\\n\")\n",
    "            if len(ids) < max_line_len:\n",
    "                ids.extend([pad_id] * (max_line_len - len(ids)))\n",
    "            ids = [str(x) for x in ids]\n",
    "            fout.write(\" \".join(ids))\n",
    "            fout.write(\"\\n\")\n",
    "            line = fin.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "I0-o2wg0gzvA"
   },
   "outputs": [],
   "source": [
    "def split_to_int_values(x):\n",
    "    return tf.string_to_number(tf.string_split([x]).values, tf.int32)\n",
    "\n",
    "\n",
    "def text_line_split_dataset(filename):\n",
    "    return tf.data.TextLineDataset(filename).map(\n",
    "        split_to_int_values)\n",
    "\n",
    "\n",
    "def words_to_ids(words, vocab):\n",
    "    ids = []\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            ids.append(vocab[word])\n",
    "        else:\n",
    "            ids.append(unk_id)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def ids_to_words(ids, rev_vocab):\n",
    "    words = \"\"\n",
    "    for id in ids:\n",
    "        words += rev_vocab[id]\n",
    "    return words\n",
    "\n",
    "\n",
    "def make_train_dataset(tweets_enc_idx_file, tweets_dec_idx_file, vocab_file,\n",
    "                       hparams):\n",
    "    sess = tf.Session()\n",
    "    # todo: skip if already exists\n",
    "    tweets_enc_idx_padded_file = \"{}.padded\".format(tweets_enc_idx_file)\n",
    "    tweets_enc_idx_len_file = \"{}.len\".format(tweets_enc_idx_file)\n",
    "\n",
    "    tweets_dec_idx_eos_file = \"{}.eos\".format(tweets_dec_idx_file)\n",
    "    tweets_dec_idx_sos_file = \"{}.sos\".format(tweets_dec_idx_file)\n",
    "    tweets_dec_idx_len_file = \"{}.len\".format(tweets_dec_idx_file)\n",
    "\n",
    "    download_file_if_necessary(tweets_enc_idx_file)\n",
    "    create_encoder_idx_padded(tweets_enc_idx_file, tweets_enc_idx_padded_file,\n",
    "                              tweets_enc_idx_len_file, hparams.encoder_length)\n",
    "    print(tweets_enc_idx_padded_file, \" created\")\n",
    "\n",
    "    download_file_if_necessary(tweets_dec_idx_file)\n",
    "    create_decoder_idx_eos(tweets_dec_idx_file, tweets_dec_idx_eos_file,\n",
    "                           hparams.decoder_length)\n",
    "    print(tweets_dec_idx_eos_file, \" created\")\n",
    "\n",
    "    create_decoder_idx_sos(tweets_dec_idx_file, tweets_dec_idx_sos_file,\n",
    "                           tweets_dec_idx_len_file, hparams.decoder_length)\n",
    "    print(tweets_dec_idx_sos_file, \" created\")\n",
    "\n",
    "    tweets_dataset = text_line_split_dataset(tweets_enc_idx_padded_file)\n",
    "    tweets_lengths_dataset = tf.data.TextLineDataset(tweets_enc_idx_len_file)\n",
    "\n",
    "    replies_sos_dataset = text_line_split_dataset(tweets_dec_idx_sos_file)\n",
    "    replies_eos_dataset = text_line_split_dataset(tweets_dec_idx_eos_file)\n",
    "    replies_sos_lengths_dataset = tf.data.TextLineDataset(\n",
    "        tweets_dec_idx_len_file)\n",
    "\n",
    "    tweets_transposed = tweets_dataset.apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(hparams.batch_size)).map(\n",
    "        lambda x: tf.transpose(x))\n",
    "    tweets_lengths = tweets_lengths_dataset.apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(hparams.batch_size))\n",
    "\n",
    "    replies_with_eos_suffix = replies_eos_dataset.apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(hparams.batch_size))\n",
    "    replies_with_sos_prefix = replies_sos_dataset.apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(hparams.batch_size)).map(\n",
    "        lambda x: tf.transpose(x))\n",
    "    replies_with_sos_suffix_lengths = replies_sos_lengths_dataset.apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(\n",
    "            hparams.batch_size))\n",
    "\n",
    "    info(\"tweets_example: {}\".format(\n",
    "        sess.run(tweets_transposed.make_one_shot_iterator().get_next())),\n",
    "        hparams)\n",
    "    info(\"tweets_lengths_example:{}\".format(\n",
    "        sess.run(tweets_lengths.make_one_shot_iterator().get_next())), hparams)\n",
    "    info(\"reply_with_eos_suffix_example:{}\".format(\n",
    "        sess.run(replies_with_eos_suffix.make_one_shot_iterator().get_next())),\n",
    "        hparams)\n",
    "    info(\"reply_with_sos_prefix_example:{}\".format(\n",
    "        sess.run(replies_with_sos_prefix.make_one_shot_iterator().get_next())),\n",
    "        hparams)\n",
    "    info(\"reply_with_sos_lengths_prefix_example:{}\".format(sess.run(\n",
    "        replies_with_sos_suffix_lengths.make_one_shot_iterator().get_next())),\n",
    "        hparams)\n",
    "\n",
    "    # Merge all using zip\n",
    "    train_feed_data = tf.data.Dataset.zip((tweets_transposed, tweets_lengths,\n",
    "                                           replies_with_eos_suffix,\n",
    "                                           replies_with_sos_prefix,\n",
    "                                           replies_with_sos_suffix_lengths)).repeat(5)\n",
    "    train_feed_data_value = sess.run(\n",
    "        train_feed_data.make_one_shot_iterator().get_next())\n",
    "    info(\"train_feed_data={}\".format(train_feed_data_value[0]), hparams)\n",
    "    info(\"train_feed_data={}\".format(train_feed_data_value[1]), hparams)\n",
    "    info(\"train_feed_data={}\".format(train_feed_data_value[2]), hparams)\n",
    "    info(\"train_feed_data={}\".format(train_feed_data_value[3]), hparams)\n",
    "\n",
    "    print(\"Dataset created\")\n",
    "\n",
    "    vocab, rev_vocab = read_vocabulary(vocab_file)\n",
    "    return train_feed_data, vocab, rev_vocab\n",
    "  \n",
    "def list_model_file(path):\n",
    "    f = open('{}/checkpoint'.format(path))\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    print(text)\n",
    "    m = re.match(r\".*ChatbotModel\\-(\\d+)\", text)\n",
    "    model_name = m.group(1)\n",
    "    all = [\"checkpoint\"]\n",
    "    all.extend([x for x in os.listdir(path) if re.search(model_name, x)])\n",
    "    return all\n",
    "\n",
    "\n",
    "def save_model_in_drive(drive, model_folder_in_drive, model_path):\n",
    "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
    "        model_folder_in_drive)}).GetList()\n",
    "    for model_file in list_model_file(model_path):\n",
    "#    for model_file in os.listdir(model_path):\n",
    "        file = drive.CreateFile({'title': model_file, \"parents\": [\n",
    "            {\"kind\": \"drive#fileLink\", \"id\": model_folder_in_drive}]})\n",
    "        file.SetContentFile(\"{}/{}\".format(model_path, model_file))\n",
    "        print(\"Uploading \", model_file, \"...\", end=\"\")\n",
    "        file.Upload()\n",
    "        print(\"done\")\n",
    "    for file in file_list:\n",
    "        f = drive.CreateFile({'id': file['id']})\n",
    "        f.Delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: apt-get: command not found\n",
      "Requirement already satisfied: mecab-python3 in /Users/higepon/.pyenv/versions/anaconda3-4.1.1/envs/py36tf1.5.0/lib/python3.6/site-packages\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y -qq mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
    "!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "DIGIT_RE = re.compile(r\"\\d\")\n",
    "\n",
    "\n",
    "def sanitize_line(line):\n",
    "    # Remove @username\n",
    "    line = re.sub(r\"@([A-Za-z0-9_]+)\", \"\", line)\n",
    "    # Remove URL\n",
    "    line = re.sub(r'https?:\\/\\/.*', \"\", line)\n",
    "    line = re.sub(DIGIT_RE, \"0\", line)\n",
    "    return line\n",
    "\n",
    "\n",
    "def create_inference_input(text, hparams, vocab):\n",
    "    inference_encoder_inputs = np.empty((hparams.encoder_length, 1),\n",
    "                                        dtype=np.int)\n",
    "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
    "    text = sanitize_line(text)\n",
    "    tagger = MeCab.Tagger(\"-Owakati\")\n",
    "    words = tagger.parse(sanitize_line(text)).split()\n",
    "    ids = words_to_ids(words, vocab)\n",
    "    ids.append(tgt_eos_id)\n",
    "    len_ids = len(ids)\n",
    "    ids.extend([pad_id] * (hparams.encoder_length - len(ids)))\n",
    "    for i in range(1):\n",
    "        inference_encoder_inputs[:, i] = np.array(ids, dtype=np.int)\n",
    "        inference_encoder_inputs_lengths[i] = len_ids\n",
    "    return inference_encoder_inputs, inference_encoder_inputs_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "H0v2MWg4Ugcj"
   },
   "outputs": [],
   "source": [
    "if colab():\n",
    "    !pip install pydrive\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "\n",
    "    def make_drive():\n",
    "        # 1. Authenticate and create the PyDrive client.\n",
    "        auth.authenticate_user()\n",
    "        gauth = GoogleAuth()\n",
    "        gauth.credentials = GoogleCredentials.get_application_default()\n",
    "        drive = GoogleDrive(gauth)\n",
    "        return drive\n",
    "    drive = make_drive()\n",
    "else:        \n",
    "    drive = None\n",
    "    def make_drive():\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hwKcEFIjU9uI"
   },
   "outputs": [],
   "source": [
    "download_file_if_necessary(p(\"tweets_train_dec_idx.txt\"))\n",
    "download_file_if_necessary(p(\"tweets_train_enc_idx.txt\"))\n",
    "\n",
    "create_decoder_idx_eos(p(\"./tweets_train_dec_idx.txt\"),\n",
    "                       p(\"./tweets_train_dec_eos_idx.txt\"),\n",
    "                       real_hparams.decoder_length)\n",
    "create_decoder_idx_sos(p(\"./tweets_train_dec_idx.txt\"),\n",
    "                       p(\"./tweets_train_dec_sos_idx.txt\"),\n",
    "                       p(\"./tweets_train_dec_sos_idx_len.txt\"),\n",
    "                       real_hparams.decoder_length)\n",
    "create_encoder_idx_padded(p(\"./tweets_train_enc_idx.txt\"),\n",
    "                          p(\"./tweets_train_enc_idx_padded.txt\"),\n",
    "                          p(\"./tweets_train_enc_idx_len.txt\"),\n",
    "                          real_hparams.encoder_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lS-jn6FN0mFx"
   },
   "outputs": [],
   "source": [
    "def print_header(name):\n",
    "    print(\"==========   {}   ========\".format(name))\n",
    "\n",
    "\n",
    "def test_small_train_loops():\n",
    "    print_header(\"make train dataset\")\n",
    "    train_feed_data, vocab, rev_vocab = make_train_dataset(\n",
    "        p(\"tweets_train_enc_idx.txt\"), p(\"tweets_train_dec_idx.txt\"),\n",
    "        p(\"vocab.txt\"),\n",
    "        real_hparams)\n",
    "    # Using same data for validation for now for this small test.\n",
    "    val_feed_data, vocab, rev_vocab = make_train_dataset(\n",
    "        p(\"tweets_train_enc_idx.txt\"), p(\"tweets_train_dec_idx.txt\"),\n",
    "        p(\"vocab.txt\"),\n",
    "        real_hparams)\n",
    "\n",
    "    print_header(\"train_loop\")\n",
    "    train_loop(train_feed_data.repeat(10), val_feed_data, vocab, rev_vocab,\n",
    "               p(\"./saved_model/real\"), real_hparams, short_loop=True)\n",
    "\n",
    "    print_header(\"train_loop_distributed_pattern\")\n",
    "    train_loop_distributed_pattern(train_feed_data.repeat(10), val_feed_data,\n",
    "                                   vocab,\n",
    "                                   rev_vocab, p(\"./saved_model/real\"),\n",
    "                                   real_hparams, short_loop=True)\n",
    "    \n",
    "    print_header(\"[source swapped] train_loop_distributed_pattern\")\n",
    "    train_loop_distributed_pattern(train_feed_data.repeat(10), val_feed_data,\n",
    "                                   vocab,\n",
    "                                   rev_vocab, p(\"./saved_model/real_swapped\"),\n",
    "                                   real_swapped_hparams, short_loop=True)    \n",
    "\n",
    "    print_header(\"train_rl_loop_distributed_pattern\")\n",
    "    train_rl_loop_distributed_pattern(train_feed_data.repeat(10), val_feed_data,\n",
    "                                      vocab,\n",
    "                                      rev_vocab, p(\"./saved_model/real\"),\n",
    "                                      p(\"./saved_model/real_rl\"), real_hparams,\n",
    "                                      short_loop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qHz9nfbXaUwZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========   make train dataset   ========\n",
      "/Users/higepon/chatbot_data/tweets_train_enc_idx.txt.padded  created\n",
      "/Users/higepon/chatbot_data/tweets_train_dec_idx.txt.eos  created\n",
      "/Users/higepon/chatbot_data/tweets_train_dec_idx.txt.sos  created\n",
      "Dataset created\n",
      "/Users/higepon/chatbot_data/tweets_train_enc_idx.txt.padded  created\n",
      "/Users/higepon/chatbot_data/tweets_train_dec_idx.txt.eos  created\n",
      "/Users/higepon/chatbot_data/tweets_train_dec_idx.txt.sos  created\n",
      "Dataset created\n",
      "==========   train_loop   ========\n",
      "generic train loop None:18lYBgKvX3AG1zhwJqP1tRYJU688U1N95\n",
      "Created fresh model.\n",
      "..==== 3 ====\n",
      "お疲れ様\n",
      "    _UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK\n",
      "    beam:_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK\n",
      "    beam:、_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK\n",
      "    beam:_UNK._UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK\n",
      "    beam:/_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK\n",
      "    beam:0_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK\n",
      "    beam:_UNK_UNK._UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK\n",
      "    beam:_UNKな_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK\n",
      "    beam:_UNKの_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK\n",
      "    beam:_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNKた\n",
      "==========   train_loop_distributed_pattern   ========\n",
      "generic train loop None:18lYBgKvX3AG1zhwJqP1tRYJU688U1N95\n",
      "train_sess= <tensorflow.python.client.session.Session object at 0x107bf2d30>\n",
      "INFO:tensorflow:Restoring parameters from /Users/higepon/chatbot_data/./saved_model/real/ChatbotModel-3\n",
      "..INFO:tensorflow:Restoring parameters from /Users/higepon/chatbot_data/./saved_model/real/ChatbotModel-6\n",
      "==== 6 ====\n",
      "お疲れ様\n",
      "    _UNK_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:_UNK_EOS_EOS_EOS_EOS_EOS_EOS_EOS_EOS_EOS_EOS_EOS\n",
      "    beam:_UNK_EOS_EOS_EOS_EOS希望希望希望希望希望希望希望\n",
      "    beam:_EOS_EOS_EOS_EOS_EOS_EOS希望希望希望希望希望希望\n",
      "    beam:_EOS希望希望希望希望希望希望希望希望希望希望希望\n",
      "    beam:_EOS_EOS希望希望希望希望希望希望希望希望希望希望\n",
      "    beam:_UNK_PAD_EOS希望希望希望希望希望希望希望希望希望\n",
      "    beam:_UNK_PAD_PAD_EOS_EOS希望希望希望希望希望希望希望\n",
      "    beam:_UNK_PAD_PAD_EOS希望希望希望希望希望希望希望希望\n",
      "    beam:_UNK_PAD_PAD_PAD_EOS希望希望希望希望希望希望希望\n",
      "==========   [source swapped] train_loop_distributed_pattern   ========\n",
      "generic train loop None:18lYBgKvX3AG1zhwJqP1tRYJU688U1N95\n",
      "train_sess= <tensorflow.python.client.session.Session object at 0x11ba0b390>\n",
      "Created fresh model.\n",
      "..INFO:tensorflow:Restoring parameters from /Users/higepon/chatbot_data/./saved_model/real_swapped/ChatbotModel-3\n",
      "==== 3 ====\n",
      "お疲れ様\n",
      "    やん_EOS\n",
      "    beam:やん_EOS希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望\n",
      "    beam:._EOS_EOS希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望\n",
      "    beam:._EOS希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望\n",
      "    beam:やんとか_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:やんの_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:チャレンジ_EOS希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望希望\n",
      "    beam:.とか_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:.の_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:やん_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "==========   train_rl_loop_distributed_pattern   ========\n",
      "<map object at 0x1162e4d68>\n",
      "INFO:tensorflow:Restoring parameters from /Users/higepon/chatbot_data/./saved_model/real/ChatbotModel-6\n",
      "INFO:tensorflow:Restoring parameters from /Users/higepon/chatbot_data/./saved_model/real/ChatbotModel-6\n",
      "..==== 9 ====\n",
      "お疲れ様ー\n",
      "    _UNK_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:_UNK_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:_UNK_UNK_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:_UNKで_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:で_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:_PAD_UNK_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:で_UNK_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:_UNK…_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n",
      "    beam:_UNK)_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD_PAD\n"
     ]
    }
   ],
   "source": [
    "test_small_train_loops()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5DNvNFIOUzfU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/higepon/chatbot_data/tweets_train_enc_idx_large.txt.padded  created\n",
      "/Users/higepon/chatbot_data/tweets_train_dec_idx_large.txt.eos  created\n",
      "/Users/higepon/chatbot_data/tweets_train_dec_idx_large.txt.sos  created\n",
      "Dataset created\n",
      "/Users/higepon/chatbot_data/tweets_val_enc_idx_large.txt.padded  created\n",
      "/Users/higepon/chatbot_data/tweets_val_dec_idx_large.txt.eos  created\n",
      "/Users/higepon/chatbot_data/tweets_val_dec_idx_large.txt.sos  created\n",
      "Dataset created\n"
     ]
    }
   ],
   "source": [
    "should_run_large_train = False\n",
    "should_run_large_train_distributed = True\n",
    "should_run_large_source_swapped_train_distributed = False\n",
    "should_run_large_train_rl = False\n",
    "\n",
    "\n",
    "if should_run_large_train or should_run_large_train_distributed or should_run_large_train_rl or should_run_large_source_swapped_train_distributed:\n",
    "    train_feed_data, vocab, rev_vocab = make_train_dataset(\n",
    "        p(\"tweets_train_enc_idx_large.txt\"),\n",
    "        p(\"tweets_train_dec_idx_large.txt\"),\n",
    "        p(\"vocab_large.txt\"), large_hparams)\n",
    "    val_feed_data, vocab, rev_vocab = make_train_dataset(\n",
    "        p(\"tweets_val_enc_idx_large.txt\"), p(\"tweets_val_dec_idx_large.txt\"),\n",
    "        p(\"vocab_large.txt\"), large_hparams)\n",
    "    val_feed_data = val_feed_data.shuffle(4096).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tPFK8YzV1s_P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generic train loop None:18lYBgKvX3AG1zhwJqP1tRYJU688U1N95\n",
      "train_sess= <tensorflow.python.client.session.Session object at 0x132fa4e48>\n",
      "Created fresh model.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-931aa4409dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     train_loop_distributed_pattern(train_feed_data, val_feed_data, vocab,\n\u001b[1;32m      7\u001b[0m                                    \u001b[0mrev_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved_model/large\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                    large_hparams, drive=drive)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshould_run_large_source_swapped_train_distributed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-23b9b64267b1>\u001b[0m in \u001b[0;36mtrain_loop_distributed_pattern\u001b[0;34m(train_feed_data, val_feed_data, vocab, rev_vocab, model_path, hparams, drive, short_loop)\u001b[0m\n\u001b[1;32m    143\u001b[0m     generic_train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n\u001b[1;32m    144\u001b[0m                        \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                        drive, short_loop)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-23b9b64267b1>\u001b[0m in \u001b[0;36mgeneric_train_loop\u001b[0;34m(train_feed_data, val_feed_data, vocab, rev_vocab, model_path, hparams, generate_models_func, inference_hook_func, drive, short_loop)\u001b[0m\n\u001b[1;32m     46\u001b[0m             global_step = model.train(train_data[enc_input_index], train_data[enc_input_length_index],\n\u001b[1;32m     47\u001b[0m                                       \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdec_input_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                                       train_data[dec_input_length_index])\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshort_loop\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f5ca75d111b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, encoder_inputs, encoder_inputs_lengths, target_labels, decoder_inputs, decoder_target_lengths, reward)\u001b[0m\n\u001b[1;32m     50\u001b[0m         }\n\u001b[1;32m     51\u001b[0m         _, global_step = self.sess.run(\n\u001b[0;32m---> 52\u001b[0;31m             [self.train_op, self.global_step], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.1.1/envs/py36tf1.5.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.1.1/envs/py36tf1.5.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.1.1/envs/py36tf1.5.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.1.1/envs/py36tf1.5.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.1.1/envs/py36tf1.5.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if should_run_large_train:\n",
    "    train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
    "               p(\"saved_model/large\"),\n",
    "               large_hparams, drive=drive)\n",
    "if should_run_large_train_distributed:\n",
    "    train_loop_distributed_pattern(train_feed_data, val_feed_data, vocab,\n",
    "                                   rev_vocab, p(\"saved_model/large\"),\n",
    "                                   large_hparams, drive=drive)\n",
    "\n",
    "if should_run_large_source_swapped_train_distributed:\n",
    "    train_loop_distributed_pattern(train_feed_data, val_feed_data, vocab,\n",
    "                                   rev_vocab, p(\"saved_model/large_swapped\"),\n",
    "                                   large_swapped_hparams, drive=drive)    \n",
    "    \n",
    "if should_run_large_train_rl:\n",
    "    train_rl_loop_distributed_pattern(train_feed_data, val_feed_data, vocab,\n",
    "                                      rev_vocab,\n",
    "                                      p(\"saved_model/large\"),\n",
    "                                      p(\"saved_model/large_rl\"), large_rl_hparams,\n",
    "                                      drive=drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1b6tvWa-rUDT"
   },
   "outputs": [],
   "source": [
    "#drive=make_drive()\n",
    "#save_model_in_drive(drive, \"./saved_model/large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FBQFWP5U_Dz7"
   },
   "outputs": [],
   "source": [
    "\n",
    "#!rm -rf  saved_model/large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "S86EmWR8J3UI"
   },
   "outputs": [],
   "source": [
    "#!ls saved_model/large_swapped/\n",
    "#!rm saved_model/large_swapped/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kZWZJcMHT_sX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/higepon/chatbot_data/saved_model/large/ChatbotModel-20585\n",
      "おはよう\n",
      "    おはよー！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
      "    beam:おはよー_EOS_EOS\n",
      "    beam:おはよー_EOSつつく\n",
      "今日は疲れた\n",
      "    _UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの\n",
      "    beam:それなwww_EOSつつくつつくつつくつつく\n",
      "    beam:それな(˙-˙)_EOS\n",
      "飲みに行こうよ\n",
      "    _UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの\n",
      "    beam:それなwww_EOSつつくつつくつつくつつく\n",
      "    beam:それな(˙-˙)_EOS\n",
      "働きたくないでござる\n",
      "    _UNK_UNK_UNK_UNK_UNK_UNK_UNK_EOS\n",
      "    beam:それなwww_EOSつつくつつくつつくつつく\n",
      "    beam:それな(˙-˙)_EOS\n",
      "名前を教えて\n",
      "    _UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの_UNKの\n",
      "    beam:それなwww_EOSつつくつつくつつくつつく\n",
      "    beam:それな(˙-˙)_EOS\n",
      "今日の東京の天気は晴のち曇で、最高気温は14度です。\n",
      "    _UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_UNK_EOS\n",
      "    beam:それなwww_EOSつつくつつくつつくつつくつつくつつくつつくつつくつつくつつくつつくつつくつつくつつくつつく\n",
      "    beam:それな！！！！！！！！！！！！！！！！_EOS\n"
     ]
    }
   ],
   "source": [
    "def test_large_model(hparams, model_path):\n",
    "    graph = tf.Graph()\n",
    "    config = tf.ConfigProto(log_device_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    tweets = [\n",
    "        \"おはよう\",\n",
    "        \"今日は疲れた\",\n",
    "        \"飲みに行こうよ\",\n",
    "        \"働きたくないでござる\",\n",
    "        \"名前を教えて\",\n",
    "        \"今日の東京の天気は晴のち曇で、最高気温は14度です。\"\n",
    "    ]\n",
    "\n",
    "    drive = make_drive()\n",
    "    download_model_data_if_necessary(drive, hparams.model_folder_in_drive,\n",
    "                                     model_path)\n",
    "\n",
    "    with graph.as_default():\n",
    "        infer_sess = tf.Session(graph=graph, config=config)\n",
    "        with tf.variable_scope('root'):\n",
    "            model = ChatbotInferenceModel(infer_sess, hparams,\n",
    "                                          model_path=model_path)\n",
    "            model.restore()\n",
    "            helper = InferenceHelper(model, vocab, rev_vocab)\n",
    "\n",
    "            for tweet in tweets:\n",
    "                helper.print_inferences(tweet)\n",
    "\n",
    "\n",
    "test_large_model(large_hparams, p(\"saved_model/large\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3jxF0tWHoBL-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/higepon/chatbot_data/saved_model/large/ChatbotModel-20585\n"
     ]
    }
   ],
   "source": [
    "def test_mutual_information(hparams, swapped_hparams):\n",
    "    graph = tf.Graph()\n",
    "    swapped_graph = tf.Graph()\n",
    "    config = tf.ConfigProto(log_device_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    drive = make_drive()\n",
    "    download_model_data_if_necessary(drive, hparams.model_folder_in_drive,\n",
    "                                     p(\"saved_model/large\"))\n",
    "    download_model_data_if_necessary(drive,\n",
    "                                     swapped_hparams.model_folder_in_drive,\n",
    "                                     p(\"saved_model/large_swapped\"))\n",
    "\n",
    "    with graph.as_default():\n",
    "        infer_sess = tf.Session(graph=graph, config=config)\n",
    "        with tf.variable_scope('root'):\n",
    "            model = ChatbotInferenceModel(infer_sess, hparams,\n",
    "                                          model_path=p(\"saved_model/large\"))\n",
    "            model.restore()\n",
    "\n",
    "    with swapped_graph.as_default():\n",
    "        swap_sess = tf.Session(graph=swapped_graph, config=config)\n",
    "        with tf.variable_scope('root'):\n",
    "            smodel = ChatbotInferenceModel(swap_sess, swapped_hparams,\n",
    "                                           model_path=p(\n",
    "                                               \"saved_model/large_swapped\"))\n",
    "            smodel.restore()\n",
    "            helper = InferenceHelper(model, vocab, rev_vocab)\n",
    "            helper.print_inferences(\"疲れた\")\n",
    "\n",
    "            shelper = InferenceHelper(smodel, vocab, rev_vocab)\n",
    "            shelper.print_inferences(\"お疲れ様\")\n",
    "\n",
    "\n",
    "large_beam_hparams = copy.deepcopy(large_hparams)\n",
    "large_beam_hparams.beam_width = 20\n",
    "test_mutual_information(large_beam_hparams, large_swapped_hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "buSmfpbFmsdT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "seq2seq.ipynb",
   "provenance": [
    {
     "file_id": "1Os9oWOWM-thM7tlXMNYfNlAC_L-l7arY",
     "timestamp": 1515554387158
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
