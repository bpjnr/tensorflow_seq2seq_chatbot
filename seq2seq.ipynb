{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/higepon/tensorflow_seq2seq_chatbot/blob/master/seq2seq.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "HWOxK9T5I8sb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Chatbot based on Seq2Seq Beam Search + Attention + Reinforcment Learning(Experimental)\n",
        "- Tensorflow 1.4.0+ is required.\n",
        "- This is based on [NMT Tutorial](https://github.com/tensorflow/nmt).\n",
        "- Experiment [notes](https://github.com/higepon/tensorflow_seq2seq_chatbot/wiki).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kK1r053SI2f9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Special commands should be located here.\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "!apt-get -qq install -y mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "\n",
        "!pip -q install git+https://github.com/mrahtz/easy-tf-log#egg=easy-tf-log[tf]\n",
        "!pip install pushbullet.py\n",
        "!pip install tweepy pyyaml\n",
        "!pip install mecab-python3\n",
        "\n",
        "def auth_google_drive():\n",
        "  # Generate creds for the Drive FUSE library.\n",
        "  if not os.path.exists('drive'):\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    creds = GoogleCredentials.get_application_default()\n",
        "    import getpass\n",
        "    !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "    vcode = getpass.getpass()\n",
        "    !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}  \n",
        "\n",
        "def mount_google_drive():\n",
        "  if not os.path.exists('drive'):\n",
        "    os.makedirs('drive', exist_ok=True)\n",
        "    !google-drive-ocamlfuse drive \n",
        "    \n",
        "def kill_docker():\n",
        "  !kill -9 -1  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "90XCqkUfbnUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "response = urllib.request.urlopen(\"https://raw.githubusercontent.com/yaroslavvb/memory_util/master/memory_util.py\")\n",
        "open(\"memory_util.py\", \"wb\").write(response.read())\n",
        "import memory_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ckr3FQqZE3Ju",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.path)\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import google\n",
        "!pip list\n",
        "# import google.auth.httplib2\n",
        "import google.auth\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WE9v1UerJMRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import google.auth\n",
        "import copy as copy\n",
        "import datetime\n",
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "import os.path\n",
        "import filecmp\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import importlib\n",
        "\n",
        "\n",
        "import MeCab\n",
        "import easy_tf_log\n",
        "import matplotlib.pyplot as plt\n",
        "import random as random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tweepy\n",
        "import yaml\n",
        "from easy_tf_log import tflog\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "import importlib\n",
        "from pushbullet import Pushbullet\n",
        "from tensorflow.python.layers import core as layers_core\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "auth.authenticate_user()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OoMe73Z51zNk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#kill_docker()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1t-QVHwn-Wdr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mM1uEwbYJPJK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth_google_drive()\n",
        "mount_google_drive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuC6wyLvVUlk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import drive.tensorflow_seq2seq_chatbot.lib.chatbot_model as sq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YPhaIY-BHV1Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reload_modules():\n",
        "  !fusermount -u drive\n",
        "  !google-drive-ocamlfuse -cc drive \n",
        "  importlib.reload(sq)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fNrRD9yOFXM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if sq.mode == sq.Mode.Test:\n",
        "    sq.test_distributed_one(enable_attention=False)\n",
        "    sq.test_distributed_one(enable_attention=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrvS_DURF5Pq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweet_small_hparams = copy.deepcopy(sq.base_hparams).override_from_dict(\n",
        "    {\n",
        "        'batch_size': 6,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 8,\n",
        "        'decoder_length': 8,\n",
        "        'num_units': 256,\n",
        "        'num_layers': 2,\n",
        "        'vocab_size': 34,\n",
        "        'embedding_size': 40,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 200,\n",
        "        'model_path': sq.ModelDirectory.tweet_small.value,\n",
        "        'learning_rate': 0.05,\n",
        "        'use_attention': True,\n",
        "    })\n",
        "\n",
        "tweet_small_swapped_hparams = copy.deepcopy(\n",
        "    tweet_small_hparams).override_from_dict(\n",
        "    {'model_path': sq.ModelDirectory.tweet_small_swapped.value})\n",
        "\n",
        "if sq.mode == sq.Mode.Test:\n",
        "    tweets_path = \"tweets_small.txt\"\n",
        "    sq.TrainDataGenerator(tweets_path, tweet_small_hparams).remove_generated()\n",
        "    trainer = sq.Trainer()\n",
        "    trainer.train_seq2seq(tweet_small_hparams, tweets_path,\n",
        "                          [\"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\"])\n",
        "    sq.test_tweets_small_swapped(tweet_small_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzh2rhEPguJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweet_large_hparams = copy.deepcopy(sq.base_hparams).override_from_dict(\n",
        "    {\n",
        "        # In typical seq2seq chatbot\n",
        "        # num_layers=3, learning_rate=0.5, batch_size=64, vocab=20000-100000, learning_rate decay is 0.99, which is taken care as default parameter in AdamOptimizer.\n",
        "        'batch_size': 64,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 28,\n",
        "        'decoder_length': 28,\n",
        "        'num_units': 1024,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': 60000,\n",
        "    # conversations.txt actually has about 70K uniq words.\n",
        "        'embedding_size': 1024,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 1000000,\n",
        "        'model_path': sq.ModelDirectory.tweet_large.value,\n",
        "        'learning_rate': 0.5,\n",
        "    # For vocab_size 50000, num_layers 3, num_units 1024, tweet_large, starting learning_rate 0.05 works well, change it t0 0.01 at perplexity 800, changed it to 0.005 at 200.\n",
        "        'learning_rate_decay': 0.99,\n",
        "        'use_attention': True,\n",
        "        # testing new restore learning rate and no USERNAME TOKEN\n",
        "    })\n",
        "\n",
        "tweet_large_swapped_hparams = copy.deepcopy(\n",
        "    tweet_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': sq.ModelDirectory.tweet_large_swapped.value\n",
        "    })\n",
        "\n",
        "#Shell.save_model_in_drive(tweet_large_hparams.model_path)\n",
        "\n",
        "if sq.mode == sq.Mode.TrainSeq2Seq:\n",
        "    print(\"train seq2seq\")\n",
        "    sq.test_tweets_large(tweet_large_hparams)\n",
        "elif sq.mode == sq.Mode.TrainSeq2SeqSwapped:\n",
        "    print(\"train seq2seq swapped\")\n",
        "    sq.test_tweets_large_swapped(tweet_large_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swhAySRidGr-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -Sl model/conversations_large*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uWLrKkcC3TXM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  \n",
        "reload_modules()\n",
        "\n",
        "conversations_large_hparams = copy.deepcopy(sq.base_hparams).override_from_dict(\n",
        "    {\n",
        "        # In typical seq2seq chatbot\n",
        "        # num_layers=3, learning_rate=0.5, batch_size=64, vocab=20000-100000, learning_rate decay is 0.99, which is taken care as default parameter in AdamOptimizer.\n",
        "        'batch_size': 128,  # of tweets should be dividable by batch_size default 64\n",
        "        'encoder_length': 28,\n",
        "        'decoder_length': 28,\n",
        "        'num_units': 1024,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': 60000,\n",
        "    # conversations.txt actually has about 70K uniq words.\n",
        "        'embedding_size': 1024,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 0,\n",
        "        'model_path': sq.ModelDirectory.conversations_large.value,\n",
        "        'learning_rate': 0.5,\n",
        "    # For vocab_size 50000, num_layers 3, num_units 1024, tweet_large, starting learning_rate 0.05 works well, change it t0 0.01 at perplexity 800, changed it to 0.005 at 200.\n",
        "        'learning_rate_decay': 0.99,\n",
        "        'use_attention': True,\n",
        "\n",
        "    })\n",
        "\n",
        "# batch_size=128, learning_rage=0.001 work very well for RL. Loss decreases as expected. enthropy didn't flat out.\n",
        "\n",
        "conversations_large_rl_hparams = copy.deepcopy(\n",
        "    conversations_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': sq.ModelDirectory.conversations_large_rl.value,\n",
        "        'num_train_steps': 2000,\n",
        "        'learning_rate': 0.001,\n",
        "        'beam_width': 3,\n",
        "    })\n",
        "\n",
        "\n",
        "conversations_large_backward_hparams = copy.deepcopy(\n",
        "    conversations_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': sq.ModelDirectory.conversations_large_backward.value,\n",
        "        'num_train_steps': 0,        \n",
        "    })\n",
        "\n",
        "resume_rl = False\n",
        "\n",
        "conversations_txt = \"conversations_large.txt\"\n",
        "sq.Shell.download_file_if_necessary(conversations_txt)\n",
        "sq.ConversationTrainDataGenerator().generate(conversations_txt)\n",
        "\n",
        "with memory_util.capture_stderr() as stderr:\n",
        "    try:\n",
        "        trainer =sq.Trainer()\n",
        "        valid_tweets = [\"さて福岡行ってきます！\", \"誰か飲みに行こう\", \"熱でてるけど、でもなんか食べなきゃーと思ってアイス買おうとしたの\",\n",
        "              \"今日のドラマ面白そう！\", \"お腹すいたー\", \"おやすみ～\", \"おはようございます。寒いですね。\",\n",
        "              \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\", \"ばいとおわ！\"]\n",
        "        trainer.train_seq2seq(conversations_large_hparams,\n",
        "                              \"conversations_large_seq2seq.txt\",\n",
        "                              valid_tweets, should_clean_saved_model=False)\n",
        "        trainer.train_seq2seq_swapped(conversations_large_backward_hparams,\n",
        "                                      \"conversations_large_seq2seq.txt\",\n",
        "                                      [\"この難にでも応用可能なひどいやつ\", \"おはようございます。明日はよろしくおねがいします。\"], vocab_path=\"conversations_large_seq2seq_vocab.txt\", should_clean_saved_model=False)\n",
        "\n",
        "        if not resume_rl:\n",
        "          sq.Shell.copy_saved_model(conversations_large_hparams, conversations_large_rl_hparams)\n",
        "        sq.Trainer().train_rl(conversations_large_rl_hparams,\n",
        "                                conversations_large_hparams,\n",
        "                                conversations_large_backward_hparams,\n",
        "                                \"conversations_large_seq2seq.txt\",\n",
        "\n",
        "                                \"conversations_large_rl.txt\",\n",
        "                                valid_tweets)\n",
        "    except Exception as e:\n",
        "        print(stderr.getvalue())\n",
        "        raise (e)\n",
        "\n",
        "!ls - lSh\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0dh_HZnDh3d9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sq.Shell.download(\"stdout.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_q-Ns9hiHMB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# N.B: This would fail if we try to download logs in the previous cell.\n",
        "# My guess is tflog is somehow locking the log file when running the cell.\n",
        "sq.Shell.download_logs(conversations_large_rl_hparams.model_path)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CErAqa_dxzQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}