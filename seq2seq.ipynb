{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/higepon/tensorflow_seq2seq_chatbot/blob/master/seq2seq.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "HWOxK9T5I8sb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Chatbot based on Seq2Seq Beam Search + Attention + Reinforcment Learning(Experimental)\n",
        "- Tensorflow 1.4.0+ is required.\n",
        "- This is based on [NMT Tutorial](https://github.com/tensorflow/nmt).\n",
        "- Experiment [notes](https://github.com/higepon/tensorflow_seq2seq_chatbot/wiki).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kK1r053SI2f9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Special commands should be located here.\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "!apt-get -qq install -y mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "\n",
        "!pip -q install git+https://github.com/mrahtz/easy-tf-log#egg=easy-tf-log[tf]\n",
        "!pip install pushbullet.py\n",
        "!pip install tweepy pyyaml\n",
        "!pip install mecab-python3\n",
        "\n",
        "def auth_google_drive():\n",
        "  # Generate creds for the Drive FUSE library.\n",
        "  if not os.path.exists('drive'):\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    creds = GoogleCredentials.get_application_default()\n",
        "    import getpass\n",
        "    !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "    vcode = getpass.getpass()\n",
        "    !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}  \n",
        "\n",
        "def mount_google_drive():\n",
        "  if not os.path.exists('drive'):\n",
        "    os.makedirs('drive', exist_ok=True)\n",
        "    !google-drive-ocamlfuse drive \n",
        "    \n",
        "def kill_docker():\n",
        "  !kill -9 -1  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "90XCqkUfbnUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "response = urllib.request.urlopen(\"https://raw.githubusercontent.com/yaroslavvb/memory_util/master/memory_util.py\")\n",
        "open(\"memory_util.py\", \"wb\").write(response.read())\n",
        "import memory_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WE9v1UerJMRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import copy as copy\n",
        "import datetime\n",
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "import os.path\n",
        "import filecmp\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import importlib\n",
        "\n",
        "\n",
        "import MeCab\n",
        "import easy_tf_log\n",
        "import matplotlib.pyplot as plt\n",
        "import random as random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tweepy\n",
        "import yaml\n",
        "from easy_tf_log import tflog\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "import importlib\n",
        "from pushbullet import Pushbullet\n",
        "from tensorflow.python.layers import core as layers_core\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "auth.authenticate_user()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OoMe73Z51zNk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#kill_docker()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mM1uEwbYJPJK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth_google_drive()\n",
        "mount_google_drive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuC6wyLvVUlk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import drive.tensorflow_seq2seq_chatbot.lib.chatbot_model as sq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YPhaIY-BHV1Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reload_modules():\n",
        "  !fusermount -u drive\n",
        "  !google-drive-ocamlfuse -cc drive \n",
        "  importlib.reload(sq)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fNrRD9yOFXM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if sq.mode == sq.Mode.Test:\n",
        "    sq.test_distributed_one(enable_attention=False)\n",
        "    sq.test_distributed_one(enable_attention=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrvS_DURF5Pq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5086
        },
        "outputId": "517e51f3-726c-4430-c563-5a8213f544af"
      },
      "cell_type": "code",
      "source": [
        "tweet_small_hparams = copy.deepcopy(sq.base_hparams).override_from_dict(\n",
        "    {\n",
        "        'batch_size': 6,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 8,\n",
        "        'decoder_length': 8,\n",
        "        'num_units': 256,\n",
        "        'num_layers': 2,\n",
        "        'vocab_size': 34,\n",
        "        'embedding_size': 40,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 200,\n",
        "        'model_path': sq.ModelDirectory.tweet_small.value,\n",
        "        'learning_rate': 0.05,\n",
        "        'use_attention': True,\n",
        "    })\n",
        "\n",
        "tweet_small_swapped_hparams = copy.deepcopy(\n",
        "    tweet_small_hparams).override_from_dict(\n",
        "    {'model_path': sq.ModelDirectory.tweet_small_swapped.value})\n",
        "\n",
        "if sq.mode == sq.Mode.Test:\n",
        "    tweets_path = \"tweets_small.txt\"\n",
        "    sq.TrainDataGenerator(tweets_path, tweet_small_hparams).remove_generated()\n",
        "    trainer = sq.Trainer()\n",
        "    trainer.train_seq2seq(tweet_small_hparams, tweets_path,\n",
        "                          [\"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\"])\n",
        "    sq.test_tweets_small_swapped(tweet_small_swapped_hparams)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    [0]おはようますますます \n",
            "    [1]おはようますますます \n",
            "    [2]おはようおはようますます \n",
            "今回もよろしくです。\n",
            "    [0]おはようますます \n",
            "    [1]おはようますます  \n",
            "    [2]おはようますますます \n",
            "average reply len=10.3\n",
            "validation loss=16.2\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small/ChatbotModel-41\n",
            "==== 41 ====\n",
            "おはようございます。寒いですね。\n",
            "    [0]おはようつかれさますます。。。\n",
            "    [1]おはようございさますます。。。\n",
            "    [2]おはようつかれさますます。。。\n",
            "さて帰ろう。明日は早い。\n",
            "    [0]おはようつかれさますます。。。\n",
            "    [1]おはようつかれさますます。。。\n",
            "    [2]おつかれさますます。。。\n",
            "今回もよろしくです。\n",
            "    [0]おはようございますます。。。。\n",
            "    [1]おはようございますます。。。。\n",
            "    [2]おはようつかれますます。。。。\n",
            "average reply len=15.0\n",
            "validation loss=13.9\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small/ChatbotModel-61\n",
            "==== 61 ====\n",
            "おはようございます。寒いですね。\n",
            "    [0]おはようございさますー。。。\n",
            "    [1]おはようございさますー。。。\n",
            "    [2]おはようつかれさますー。。。\n",
            "さて帰ろう。明日は早い。\n",
            "    [0]おつかれさますー。。。\n",
            "    [1]おつかれさますー。。。\n",
            "    [2]おつかれさまー。。。\n",
            "今回もよろしくです。\n",
            "    [0]おはようこそよろしくます。。。。\n",
            "    [1]おはようこそよろしくます。。。。\n",
            "    [2]おはようこそますます。。。。\n",
            "average reply len=13.7\n",
            "validation loss=12.1\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small/ChatbotModel-81\n",
            "==== 81 ====\n",
            "おはようございます。寒いですね。\n",
            "    [0]おつかれさまー。。。\n",
            "    [1]おつかれさまー。。。\n",
            "    [2]おつかれさ！ー。。。\n",
            "さて帰ろう。明日は早い。\n",
            "    [0]おつかれさまー。。。\n",
            "    [1]おつかれさまー。。。\n",
            "    [2]おつかれさ！ー。。。\n",
            "今回もよろしくです。\n",
            "    [0]こちらこそよろしくお願いし。。。\n",
            "    [1]こちらこそよろしくお願いし。。。\n",
            "    [2]こちらこそよろしくお願いお願い。。。\n",
            "average reply len=12.0\n",
            "validation loss=9.9\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small/ChatbotModel-101\n",
            "==== 101 ====\n",
            "おはようございます。寒いですね。\n",
            "    [0]おつかれます！ー。。 \n",
            "    [1]おつかれます！ー。。 \n",
            "    [2]おつかれます！ー。。気\n",
            "さて帰ろう。明日は早い。\n",
            "    [0]おつかれます！ー。。気\n",
            "    [1]おつかれます！ー。。気\n",
            "    [2]おつかれさ！ー。。気\n",
            "今回もよろしくです。\n",
            "    [0]こちらこそよろしくお願いししますます\n",
            "    [1]こちらこそよろしくお願いししますます\n",
            "    [2]こちらこそよろしくお願いしますますます\n",
            "average reply len=13.3\n",
            "validation loss=7.5\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small/ChatbotModel-121\n",
            "==== 121 ====\n",
            "おはようございます。寒いですね。\n",
            "    [0]おつかれさまー。気気\n",
            "    [1]おつかれさまー。気気\n",
            "    [2]おつかれさ！ー。気気\n",
            "さて帰ろう。明日は早い。\n",
            "    [0]おつかれさまー。気気\n",
            "    [1]おつかれさまー。気気\n",
            "    [2]おつかれさまー気気気\n",
            "今回もよろしくです。\n",
            "    [0]こちらこそよろしくお願いしし。。\n",
            "    [1]こちらこそよろしくお願いしし。。\n",
            "    [2]こちらこそよろしくお願いします。。\n",
            "average reply len=12.0\n",
            "validation loss=6.5\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small/ChatbotModel-141\n",
            "==== 141 ====\n",
            "おはようございます。寒いですね。\n",
            "    [0]おつかれさまー。気気\n",
            "    [1]おつかれさまー。気気\n",
            "    [2]おつかれさまー。。気\n",
            "さて帰ろう。明日は早い。\n",
            "    [0]おつかれさまー。気気\n",
            "    [1]おつかれさまー。気気\n",
            "    [2]おつかれさまー気気気\n",
            "今回もよろしくです。\n",
            "    [0]こちらこそよろしくお願いします。。\n",
            "    [1]こちらこそよろしくお願いします。。\n",
            "    [2]こちらこそよろしくお願いしし。。\n",
            "average reply len=12.3\n",
            "validation loss=5.8\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small/ChatbotModel-161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "==== 161 ====\n",
            "おはようございます。寒いですね。\n",
            "    [0]おつかれさ！ \n",
            "    [1]おつかれさ！ \n",
            "    [2]おつかれます！ \n",
            "さて帰ろう。明日は早い。\n",
            "    [0]おつかれさま \n",
            "    [1]おつかれさま  \n",
            "    [2]おつかれさまー \n",
            "今回もよろしくです。\n",
            "    [0]こちらこそよろしくお願いし。 \n",
            "    [1]こちらこそよろしくお願いし  \n",
            "    [2]こちらこそよろしくお願いし。 \n",
            "average reply len=9.7\n",
            "validation loss=5.1\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small/ChatbotModel-181\n",
            "==== 181 ====\n",
            "おはようございます。寒いですね。\n",
            "    [0]おはようございます！ー。気気\n",
            "    [1]おはようございます！ー。気気\n",
            "    [2]おはようございさ！ー。気気\n",
            "さて帰ろう。明日は早い。\n",
            "    [0]おつかれさまー。気気\n",
            "    [1]おつかれさまー。気気\n",
            "    [2]おつかれさまー。。気\n",
            "今回もよろしくです。\n",
            "    [0]こちらこそよろしくお願いします。。\n",
            "    [1]こちらこそよろしくお願いします。。\n",
            "    [2]こちらこそよろしくお願いしし。。\n",
            "average reply len=13.7\n",
            "validation loss=4.1\n",
            "...................===== Train Seq2Seq tweets_small_swapped.txt ====\n",
            "hparams= {'machine': 'client1', 'batch_size': 6, 'num_units': 256, 'num_layers': 2, 'vocab_size': 34, 'embedding_size': 40, 'learning_rate': 0.05, 'learning_rate_decay': 0.99, 'use_attention': True, 'encoder_length': 8, 'decoder_length': 8, 'max_gradient_norm': 5.0, 'beam_width': 2, 'num_train_steps': 200, 'model_path': 'model/tweet_small_swapped'}\n",
            "generating enc and dec files...\n",
            "generating vocab file...\n",
            "loading vocab...\n",
            "generating id files...\n",
            "generating padded input file...\n",
            "generating dec eos/sos files...\n",
            "done\n",
            "Downloading model files...\n",
            "done\n",
            "$$$ GPU ENABLED $$$\n",
            "$$$ GPU ENABLED $$$\n",
            "$$$ GPU ENABLED $$$\n",
            "Created fresh model.\n",
            "....................INFO:tensorflow:Restoring parameters from model/tweet_small_swapped/ChatbotModel-21\n",
            "==== 21 ====\n",
            "@higepon おはようございます！\n",
            "    [0]。。。。 \n",
            "    [1]。。。。  \n",
            "    [2]。。。。。 \n",
            "おつかれさまー。気をつけて。\n",
            "    [0]さて。。。。 \n",
            "    [1]さて。。。。  \n",
            "    [2]さて。。。。。 \n",
            "こちらこそよろしくお願いします。\n",
            "    [0]さて。。。。 \n",
            "    [1]さて。。。。 \n",
            "    [2]帰ろ。。。。 \n",
            "average reply len=6.3\n",
            "validation loss=16.7\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small_swapped/ChatbotModel-41\n",
            "==== 41 ====\n",
            "@higepon おはようございます！\n",
            "    [0]さて。。。。ですねね\n",
            "    [1]さても。。。ですねね\n",
            "    [2]さて。。。。ですねね\n",
            "おつかれさまー。気をつけて。\n",
            "    [0]さて帰ろ。。。。早い早い\n",
            "    [1]さて帰ろ。。。。早い早い\n",
            "    [2]さてさて。。。。早い早い\n",
            "こちらこそよろしくお願いします。\n",
            "    [0]さて帰ろ。。。。早い早い\n",
            "    [1]さて帰ろ。。。。早い早い\n",
            "    [2]さても。。。。早い早い\n",
            "average reply len=11.3\n",
            "validation loss=13.8\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small_swapped/ChatbotModel-61\n",
            "==== 61 ====\n",
            "@higepon おはようございます！\n",
            "    [0]今回ござい。。。ですねね\n",
            "    [1]今回ござい。。。ですねね\n",
            "    [2]今回も。。。ですねね\n",
            "おつかれさまー。気をつけて。\n",
            "    [0]さて帰ろう。。。早い早い\n",
            "    [1]さて帰ろう。。。早い早い\n",
            "    [2]さて帰ろう。。。は早い\n",
            "こちらこそよろしくお願いします。\n",
            "    [0]さてもう。。。早い早い\n",
            "    [1]さてもう。。。早い早い\n",
            "    [2]さてもよろしく。。。早い早い\n",
            "average reply len=11.7\n",
            "validation loss=11.8\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small_swapped/ChatbotModel-81\n",
            "==== 81 ====\n",
            "@higepon おはようございます！\n",
            "    [0]おはようござい。。。ですねね\n",
            "    [1]おはようござい。。。ですねね\n",
            "    [2]おはようござい。。ですですねね\n",
            "おつかれさまー。気をつけて。\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    [0]さて帰ろう。。。早い早い\n",
            "    [1]さて帰ろう。。。早い早い\n",
            "    [2]さて帰ろう。。は早い早い\n",
            "こちらこそよろしくお願いします。\n",
            "    [0]今回もよろしく。。。早い \n",
            "    [1]今回もよろしく。。。早い \n",
            "    [2]さてもよろしく。。。早い \n",
            "average reply len=13.0\n",
            "validation loss=9.8\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small_swapped/ChatbotModel-101\n",
            "==== 101 ====\n",
            "@higepon おはようございます！\n",
            "    [0]おはようございます。です \n",
            "    [1]おはようございます。です \n",
            "    [2]おはようございます。寒い \n",
            "おつかれさまー。気をつけて。\n",
            "    [0]さて帰ろう。。 \n",
            "    [1]さて帰ろう。。 \n",
            "    [2]さて帰ろよろしく。。 \n",
            "こちらこそよろしくお願いします。\n",
            "    [0]今回もよろしく。。 \n",
            "    [1]今回もよろしく。。 \n",
            "    [2]今回も。。。 \n",
            "average reply len=10.3\n",
            "validation loss=8.0\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small_swapped/ChatbotModel-121\n",
            "==== 121 ====\n",
            "@higepon おはようございます！\n",
            "    [0]おはようございます。です \n",
            "    [1]おはようございます。です \n",
            "    [2]おはようございます。寒い \n",
            "おつかれさまー。気をつけて。\n",
            "    [0]さて帰ろう明日明日は早い \n",
            "    [1]さて帰ろう明日明日は  \n",
            "    [2]さて帰ろう明日明日は早い \n",
            "こちらこそよろしくお願いします。\n",
            "    [0]今回もよろしく。明日 \n",
            "    [1]今回もよろしく。明日 \n",
            "    [2]今回もよろしく。は \n",
            "average reply len=12.3\n",
            "validation loss=6.5\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small_swapped/ChatbotModel-141\n",
            "==== 141 ====\n",
            "@higepon おはようございます！\n",
            "    [0]おはようございます。寒いですねね\n",
            "    [1]おはようございます。寒いですねね\n",
            "    [2]おはようございます。寒い寒いねね\n",
            "おつかれさまー。気をつけて。\n",
            "    [0]さて帰ろう。。は早い早い\n",
            "    [1]さて帰ろう。。は早い早い\n",
            "    [2]さて帰ろう。明日は早い早い\n",
            "こちらこそよろしくお願いします。\n",
            "    [0]今回もよろしく。。です \n",
            "    [1]今回もよろしく。。です \n",
            "    [2]今回もよろしく。ですです \n",
            "average reply len=13.3\n",
            "validation loss=5.4\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small_swapped/ChatbotModel-161\n",
            "==== 161 ====\n",
            "@higepon おはようございます！\n",
            "    [0]おはようございます。寒い寒いねね\n",
            "    [1]おはようございます。寒い寒いねね\n",
            "    [2]おはようございます。。寒いねね\n",
            "おつかれさまー。気をつけて。\n",
            "    [0]さて帰ろう。。は早い早い\n",
            "    [1]さて帰ろう。。は早い早い\n",
            "    [2]さて帰ろう。明日は早い早い\n",
            "こちらこそよろしくお願いします。\n",
            "    [0]今回もよろしく。。 \n",
            "    [1]今回もよろしく。。  \n",
            "    [2]今回もよろしく。。。 \n",
            "average reply len=12.7\n",
            "validation loss=4.5\n",
            "...................INFO:tensorflow:Restoring parameters from model/tweet_small_swapped/ChatbotModel-181\n",
            "==== 181 ====\n",
            "@higepon おはようございます！\n",
            "    [0]おはようございます。寒いですねね\n",
            "    [1]おはようございます。寒いですねね\n",
            "    [2]おはようございます。寒いですですね\n",
            "おつかれさまー。気をつけて。\n",
            "    [0]さて帰ろう。明日は早い早い\n",
            "    [1]さて帰ろう。明日は早い早い\n",
            "    [2]さて帰ろう。明日はは早い\n",
            "こちらこそよろしくお願いします。\n",
            "    [0]今回もよろしくです。 \n",
            "    [1]今回もよろしくです。  \n",
            "    [2]今回もよろしくです。。 \n",
            "average reply len=13.3\n",
            "validation loss=3.6\n",
            "..................."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tzh2rhEPguJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweet_large_hparams = copy.deepcopy(sq.base_hparams).override_from_dict(\n",
        "    {\n",
        "        # In typical seq2seq chatbot\n",
        "        # num_layers=3, learning_rate=0.5, batch_size=64, vocab=20000-100000, learning_rate decay is 0.99, which is taken care as default parameter in AdamOptimizer.\n",
        "        'batch_size': 64,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 28,\n",
        "        'decoder_length': 28,\n",
        "        'num_units': 1024,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': 60000,\n",
        "    # conversations.txt actually has about 70K uniq words.\n",
        "        'embedding_size': 1024,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 1000000,\n",
        "        'model_path': sq.ModelDirectory.tweet_large.value,\n",
        "        'learning_rate': 0.5,\n",
        "    # For vocab_size 50000, num_layers 3, num_units 1024, tweet_large, starting learning_rate 0.05 works well, change it t0 0.01 at perplexity 800, changed it to 0.005 at 200.\n",
        "        'learning_rate_decay': 0.99,\n",
        "        'use_attention': True,\n",
        "        # testing new restore learning rate and no USERNAME TOKEN\n",
        "    })\n",
        "\n",
        "tweet_large_swapped_hparams = copy.deepcopy(\n",
        "    tweet_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': sq.ModelDirectory.tweet_large_swapped.value\n",
        "    })\n",
        "\n",
        "#Shell.save_model_in_drive(tweet_large_hparams.model_path)\n",
        "\n",
        "if sq.mode == sq.Mode.TrainSeq2Seq:\n",
        "    print(\"train seq2seq\")\n",
        "    sq.test_tweets_large(tweet_large_hparams)\n",
        "elif sq.mode == sq.Mode.TrainSeq2SeqSwapped:\n",
        "    print(\"train seq2seq swapped\")\n",
        "    sq.test_tweets_large_swapped(tweet_large_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swhAySRidGr-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "fe241092-6359-44d9-cd3d-1f99ac32b66a"
      },
      "cell_type": "code",
      "source": [
        "!ls -Sl model/conversations_large*"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model/conversations_large:\r\n",
            "total 732908\r\n",
            "-rw-r--r-- 1 root root 722305028 Jun 28 12:00 ChatbotModel-27584.data-00000-of-00001\r\n",
            "-rw-r--r-- 1 root root   5612352 Jun 28 12:00 events.out.tfevents.1530109067.1cfeca23cb88\r\n",
            "-rw-r--r-- 1 root root   5592260 Jun 28 12:00 events.out.tfevents.1529845774.4a4f7239805d\r\n",
            "-rw-r--r-- 1 root root   5128307 Jun 28 12:00 events.out.tfevents.1530177405.67a0b8527e19\r\n",
            "-rw-r--r-- 1 root root   4991370 Jun 28 12:00 events.out.tfevents.1530186319.67a0b8527e19\r\n",
            "-rw-r--r-- 1 root root   3984970 Aug  3 23:01 events.out.tfevents.1533337297.5221847541c6\r\n",
            "-rw-r--r-- 1 root root   2667400 Jun 28 12:00 ChatbotModel-27584.meta\r\n",
            "-rw-r--r-- 1 root root     78628 Jun 28 12:00 events.out.tfevents.1530109115.1cfeca23cb88\r\n",
            "-rw-r--r-- 1 root root     76708 Jun 28 12:00 events.out.tfevents.1529845842.4a4f7239805d\r\n",
            "-rw-r--r-- 1 root root     18468 Jun 28 12:00 events.out.tfevents.1530177455.67a0b8527e19\r\n",
            "-rw-r--r-- 1 root root      1800 Jun 28 12:00 events.out.tfevents.1530186369.67a0b8527e19\r\n",
            "-rw-r--r-- 1 root root       999 Jun 28 12:00 ChatbotModel-27584.index\r\n",
            "-rw-r--r-- 1 root root        93 Jun 28 12:00 checkpoint\r\n",
            "\r\n",
            "model/conversations_large_backward:\r\n",
            "total 748512\r\n",
            "-rw-r--r-- 1 root root 722305028 Jun 28 09:35 ChatbotModel-45146.data-00000-of-00001\r\n",
            "-rw-r--r-- 1 root root   5753311 Jun 28 09:36 events.out.tfevents.1530085444.7818e1a66454\r\n",
            "-rw-r--r-- 1 root root   5416351 Jun 28 09:36 events.out.tfevents.1530057799.6a33c7a0f7c8\r\n",
            "-rw-r--r-- 1 root root   5411101 Jun 28 09:35 events.out.tfevents.1530016290.b5abe5faf728\r\n",
            "-rw-r--r-- 1 root root   5280113 Jun 28 09:35 events.out.tfevents.1529996517.b5abe5faf728\r\n",
            "-rw-r--r-- 1 root root   5263817 Jun 28 09:35 events.out.tfevents.1529931211.ab92cd832bf9\r\n",
            "-rw-r--r-- 1 root root   5068574 Jun 28 09:35 events.out.tfevents.1529926214.ab92cd832bf9\r\n",
            "-rw-r--r-- 1 root root   4984037 Jun 28 09:35 events.out.tfevents.1529926847.afb645c5f2db\r\n",
            "-rw-r--r-- 1 root root   3985126 Aug  3 23:05 events.out.tfevents.1533337529.5221847541c6\r\n",
            "-rw-r--r-- 1 root root   2667480 Jun 28 09:35 ChatbotModel-45146.meta\r\n",
            "-rw-r--r-- 1 root root     95268 Jun 28 09:14 events.out.tfevents.1530085491.7818e1a66454\r\n",
            "-rw-r--r-- 1 root root     53796 Jun 28 09:14 events.out.tfevents.1530057846.6a33c7a0f7c8\r\n",
            "-rw-r--r-- 1 root root     53668 Jun 28 09:14 events.out.tfevents.1530016339.b5abe5faf728\r\n",
            "-rw-r--r-- 1 root root     37796 Jun 28 09:14 events.out.tfevents.1529996564.b5abe5faf728\r\n",
            "-rw-r--r-- 1 root root     36004 Jun 28 09:14 events.out.tfevents.1529931261.ab92cd832bf9\r\n",
            "-rw-r--r-- 1 root root     11124 Jun 28 09:14 events.out.tfevents.1529926258.ab92cd832bf9\r\n",
            "-rw-r--r-- 1 root root       999 Jun 28 09:14 ChatbotModel-45146.index\r\n",
            "-rw-r--r-- 1 root root        93 Jun 28 09:14 checkpoint\r\n",
            "\r\n",
            "model/conversations_large_rl:\r\n",
            "total 1419956\r\n",
            "-rw-r--r-- 1 root root 722305028 Jun 28 12:00 ChatbotModel-27584.data-00000-of-00001\r\n",
            "-rw-r--r-- 1 root root 722305028 Aug  4 05:12 ChatbotModel-28485.data-00000-of-00001\r\n",
            "-rw-r--r-- 1 root root   3653017 Aug  3 23:10 events.out.tfevents.1533337833.5221847541c6\r\n",
            "-rw-r--r-- 1 root root   2667400 Jun 28 12:00 ChatbotModel-27584.meta\r\n",
            "-rw-r--r-- 1 root root   2148685 Aug  4 05:12 ChatbotModel-28485.meta\r\n",
            "-rw-r--r-- 1 root root    913664 Aug  4 05:51 events.out.tfevents.1533337835.5221847541c6\r\n",
            "-rw-r--r-- 1 root root       999 Jun 28 12:00 ChatbotModel-27584.index\r\n",
            "-rw-r--r-- 1 root root       999 Aug  4 05:12 ChatbotModel-28485.index\r\n",
            "-rw-r--r-- 1 root root        93 Aug  4 05:12 checkpoint\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uWLrKkcC3TXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df8d2477-589e-4d16-80ce-bceecaed104c"
      },
      "cell_type": "code",
      "source": [
        "  \n",
        "reload_modules()\n",
        "\n",
        "conversations_large_hparams = copy.deepcopy(sq.base_hparams).override_from_dict(\n",
        "    {\n",
        "        # In typical seq2seq chatbot\n",
        "        # num_layers=3, learning_rate=0.5, batch_size=64, vocab=20000-100000, learning_rate decay is 0.99, which is taken care as default parameter in AdamOptimizer.\n",
        "        'batch_size': 128,  # of tweets should be dividable by batch_size default 64\n",
        "        'encoder_length': 28,\n",
        "        'decoder_length': 28,\n",
        "        'num_units': 1024,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': 60000,\n",
        "    # conversations.txt actually has about 70K uniq words.\n",
        "        'embedding_size': 1024,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 0,\n",
        "        'model_path': sq.ModelDirectory.conversations_large.value,\n",
        "        'learning_rate': 0.5,\n",
        "    # For vocab_size 50000, num_layers 3, num_units 1024, tweet_large, starting learning_rate 0.05 works well, change it t0 0.01 at perplexity 800, changed it to 0.005 at 200.\n",
        "        'learning_rate_decay': 0.99,\n",
        "        'use_attention': True,\n",
        "\n",
        "    })\n",
        "\n",
        "# batch_size=128, learning_rage=0.001 work very well for RL. Loss decreases as expected. enthropy didn't flat out.\n",
        "\n",
        "conversations_large_rl_hparams = copy.deepcopy(\n",
        "    conversations_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': sq.ModelDirectory.conversations_large_rl.value,\n",
        "        'num_train_steps': 1000,\n",
        "        'learning_rate': 0.001,\n",
        "        'beam_width': 3,\n",
        "    })\n",
        "\n",
        "\n",
        "conversations_large_backward_hparams = copy.deepcopy(\n",
        "    conversations_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': sq.ModelDirectory.conversations_large_backward.value,\n",
        "        'num_train_steps': 0,        \n",
        "    })\n",
        "\n",
        "\n",
        "conversations_txt = \"conversations_large.txt\"\n",
        "sq.Shell.download_file_if_necessary(conversations_txt)\n",
        "sq.ConversationTrainDataGenerator().generate(conversations_txt)\n",
        "\n",
        "with memory_util.capture_stderr() as stderr:\n",
        "    try:\n",
        "        trainer =sq.Trainer()\n",
        "        valid_tweets = [\"さて福岡行ってきます！\", \"誰か飲みに行こう\", \"熱でてるけど、でもなんか食べなきゃーと思ってアイス買おうとしたの\",\n",
        "              \"今日のドラマ面白そう！\", \"お腹すいたー\", \"おやすみ～\", \"おはようございます。寒いですね。\",\n",
        "              \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\", \"ばいとおわ！\"]\n",
        "        trainer.train_seq2seq(conversations_large_hparams,\n",
        "                              \"conversations_large_seq2seq.txt\",\n",
        "                              valid_tweets, should_clean_saved_model=False)\n",
        "        trainer.train_seq2seq_swapped(conversations_large_backward_hparams,\n",
        "                                      \"conversations_large_seq2seq.txt\",\n",
        "                                      [\"この難にでも応用可能なひどいやつ\", \"おはようございます。明日はよろしくおねがいします。\"], vocab_path=\"conversations_large_seq2seq_vocab.txt\", should_clean_saved_model=False)\n",
        "\n",
        "        sq.Shell.copy_saved_model(conversations_large_hparams, conversations_large_rl_hparams)\n",
        "        sq.Trainer().train_rl(conversations_large_rl_hparams,\n",
        "                                conversations_large_hparams,\n",
        "                                conversations_large_backward_hparams,\n",
        "                                \"conversations_large_seq2seq.txt\",\n",
        "\n",
        "                                \"conversations_large_rl.txt\",\n",
        "                                valid_tweets)\n",
        "    except Exception as e:\n",
        "        print(stderr.getvalue())\n",
        "        raise (e)\n",
        "\n",
        "!ls - lSh\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clearing cache...done\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0dh_HZnDh3d9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sq.Shell.download(\"stdout.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_q-Ns9hiHMB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# N.B: This would fail if we try to download logs in the previous cell.\n",
        "# My guess is tflog is somehow locking the log file when running the cell.\n",
        "sq.Shell.download_logs(conversations_large_rl_hparams.model_path)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CErAqa_dxzQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}